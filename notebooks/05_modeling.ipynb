{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 - Modeling & Evaluation\n",
        "\n",
        "Train multiple models (LightGBM, CatBoost, TabNet) to predict podium probability (top-3 finish).\n",
        "\n",
        "**Models:**\n",
        "1. **LightGBM**: Primary tree-based ensemble with categorical feature support\n",
        "2. **CatBoost**: Alternative tree ensemble with built-in categorical handling\n",
        "3. **TabNet**: Deep learning model with attention-based feature selection\n",
        "\n",
        "**Train/Val/Test Split:**\n",
        "- Train: 1994-2022\n",
        "- Validation: 2023\n",
        "- Test: 2024\n",
        "\n",
        "**Input:** `data/processed/features.csv`  \n",
        "**Output:** `models/best_podium_model.pkl`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    brier_score_loss,\n",
        "    log_loss,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Models\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "    print(\"Warning: LightGBM not available\")\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CATBOOST_AVAILABLE = False\n",
        "    print(\"Warning: CatBoost not available\")\n",
        "\n",
        "try:\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "    TABNET_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TABNET_AVAILABLE = False\n",
        "    print(\"Warning: TabNet not available\")\n",
        "\n",
        "# Set up paths\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "PROCESSED_ROOT = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "MODELS_ROOT = PROJECT_ROOT / \"models\"\n",
        "MODELS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load features\n",
        "features = pd.read_csv(PROCESSED_ROOT / \"features.csv\")\n",
        "features['date'] = pd.to_datetime(features['date'], errors='coerce')\n",
        "\n",
        "print(f\"Loaded features: {features.shape}\")\n",
        "print(f\"Date range: {features['date'].min()} to {features['date'].max()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "Prepare train/val/test splits and handle missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = features.drop(columns=['podium', 'raceId', 'driverId', 'date', 'year'], errors='ignore')\n",
        "y = features['podium']\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str).fillna('Unknown'))\n",
        "\n",
        "# Fill missing values with median for numeric, mode for categorical\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in [np.float64, np.int64]:\n",
        "        X[col] = X[col].fillna(X[col].median())\n",
        "    else:\n",
        "        X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "print(f\"Podium rate: {y.mean():.2%}\")\n",
        "\n",
        "# Train/Val/Test split\n",
        "train_mask = features['year'] < 2023\n",
        "val_mask = features['year'] == 2023\n",
        "test_mask = features['year'] == 2024\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val, y_val = X[val_mask], y[val_mask]\n",
        "X_test, y_test = X[test_mask], y[test_mask]\n",
        "\n",
        "print(f\"\\nTrain: {len(X_train):,} samples ({features[train_mask]['year'].min()}-{features[train_mask]['year'].max()})\")\n",
        "print(f\"Val: {len(X_val):,} samples (2023)\")\n",
        "print(f\"Test: {len(X_test):,} samples (2024)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Model 1: LightGBM\n",
        "\n",
        "Train LightGBM with categorical feature support.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if LIGHTGBM_AVAILABLE:\n",
        "    # LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=7,\n",
        "        num_leaves=31,\n",
        "        objective='binary',\n",
        "        metric='binary_logloss',\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    )\n",
        "    \n",
        "    lgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
        "    )\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
        "    y_test_pred_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    lgb_val_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_val, y_val_pred_lgb),\n",
        "        'pr_auc': average_precision_score(y_val, y_val_pred_lgb),\n",
        "        'brier': brier_score_loss(y_val, y_val_pred_lgb),\n",
        "        'log_loss': log_loss(y_val, y_val_pred_lgb)\n",
        "    }\n",
        "    \n",
        "    lgb_test_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_test, y_test_pred_lgb),\n",
        "        'pr_auc': average_precision_score(y_test, y_test_pred_lgb),\n",
        "        'brier': brier_score_loss(y_test, y_test_pred_lgb),\n",
        "        'log_loss': log_loss(y_test, y_test_pred_lgb)\n",
        "    }\n",
        "    \n",
        "    print(\"LightGBM Results:\")\n",
        "    print(f\"  Val ROC-AUC: {lgb_val_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Val PR-AUC: {lgb_val_metrics['pr_auc']:.4f}\")\n",
        "    print(f\"  Test ROC-AUC: {lgb_test_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Test PR-AUC: {lgb_test_metrics['pr_auc']:.4f}\")\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance_lgb = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': lgb_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTop 10 Features (LightGBM):\")\n",
        "    print(feature_importance_lgb.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"LightGBM not available - skipping\")\n",
        "    lgb_model = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Model 2: CatBoost\n",
        "\n",
        "Train CatBoost with built-in categorical handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CATBOOST_AVAILABLE:\n",
        "    # CatBoost model\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=500,\n",
        "        learning_rate=0.05,\n",
        "        depth=7,\n",
        "        loss_function='Logloss',\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    cat_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        early_stopping_rounds=50\n",
        "    )\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred_cat = cat_model.predict_proba(X_val)[:, 1]\n",
        "    y_test_pred_cat = cat_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    cat_val_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_val, y_val_pred_cat),\n",
        "        'pr_auc': average_precision_score(y_val, y_val_pred_cat),\n",
        "        'brier': brier_score_loss(y_val, y_val_pred_cat),\n",
        "        'log_loss': log_loss(y_val, y_val_pred_cat)\n",
        "    }\n",
        "    \n",
        "    cat_test_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_test, y_test_pred_cat),\n",
        "        'pr_auc': average_precision_score(y_test, y_test_pred_cat),\n",
        "        'brier': brier_score_loss(y_test, y_test_pred_cat),\n",
        "        'log_loss': log_loss(y_test, y_test_pred_cat)\n",
        "    }\n",
        "    \n",
        "    print(\"CatBoost Results:\")\n",
        "    print(f\"  Val ROC-AUC: {cat_val_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Val PR-AUC: {cat_val_metrics['pr_auc']:.4f}\")\n",
        "    print(f\"  Test ROC-AUC: {cat_test_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Test PR-AUC: {cat_test_metrics['pr_auc']:.4f}\")\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance_cat = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': cat_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTop 10 Features (CatBoost):\")\n",
        "    print(feature_importance_cat.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"CatBoost not available - skipping\")\n",
        "    cat_model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Model 3: TabNet\n",
        "\n",
        "Train TabNet deep learning model with attention-based feature selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TABNET_AVAILABLE:\n",
        "    import torch\n",
        "    # TabNet model\n",
        "    tabnet_model = TabNetClassifier(\n",
        "        n_d=64,\n",
        "        n_a=64,\n",
        "        n_steps=5,\n",
        "        gamma=1.5,\n",
        "        n_independent=2,\n",
        "        n_shared=2,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=2e-2),\n",
        "        scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        mask_type='entmax',\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    tabnet_model.fit(\n",
        "        X_train.values, y_train.values,\n",
        "        eval_set=[(X_val.values, y_val.values)],\n",
        "        eval_metric=['auc'],\n",
        "        max_epochs=100,\n",
        "        patience=20,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128\n",
        "    )\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred_tab = tabnet_model.predict_proba(X_val.values)[:, 1]\n",
        "    y_test_pred_tab = tabnet_model.predict_proba(X_test.values)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    tabnet_val_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_val, y_val_pred_tab),\n",
        "        'pr_auc': average_precision_score(y_val, y_val_pred_tab),\n",
        "        'brier': brier_score_loss(y_val, y_val_pred_tab),\n",
        "        'log_loss': log_loss(y_val, y_val_pred_tab)\n",
        "    }\n",
        "    \n",
        "    tabnet_test_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_test, y_test_pred_tab),\n",
        "        'pr_auc': average_precision_score(y_test, y_test_pred_tab),\n",
        "        'brier': brier_score_loss(y_test, y_test_pred_tab),\n",
        "        'log_loss': log_loss(y_test, y_test_pred_tab)\n",
        "    }\n",
        "    \n",
        "    print(\"TabNet Results:\")\n",
        "    print(f\"  Val ROC-AUC: {tabnet_val_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Val PR-AUC: {tabnet_val_metrics['pr_auc']:.4f}\")\n",
        "    print(f\"  Test ROC-AUC: {tabnet_test_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Test PR-AUC: {tabnet_test_metrics['pr_auc']:.4f}\")\n",
        "else:\n",
        "    print(\"TabNet not available - skipping\")\n",
        "    tabnet_model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Model Comparison\n",
        "\n",
        "Compare all models and select the best one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models on validation set\n",
        "model_comparison = []\n",
        "\n",
        "if LIGHTGBM_AVAILABLE:\n",
        "    model_comparison.append({\n",
        "        'model': 'LightGBM',\n",
        "        'val_roc_auc': lgb_val_metrics['roc_auc'],\n",
        "        'val_pr_auc': lgb_val_metrics['pr_auc'],\n",
        "        'test_roc_auc': lgb_test_metrics['roc_auc'],\n",
        "        'test_pr_auc': lgb_test_metrics['pr_auc']\n",
        "    })\n",
        "\n",
        "if CATBOOST_AVAILABLE:\n",
        "    model_comparison.append({\n",
        "        'model': 'CatBoost',\n",
        "        'val_roc_auc': cat_val_metrics['roc_auc'],\n",
        "        'val_pr_auc': cat_val_metrics['pr_auc'],\n",
        "        'test_roc_auc': cat_test_metrics['roc_auc'],\n",
        "        'test_pr_auc': cat_test_metrics['pr_auc']\n",
        "    })\n",
        "\n",
        "if TABNET_AVAILABLE:\n",
        "    model_comparison.append({\n",
        "        'model': 'TabNet',\n",
        "        'val_roc_auc': tabnet_val_metrics['roc_auc'],\n",
        "        'val_pr_auc': tabnet_val_metrics['pr_auc'],\n",
        "        'test_roc_auc': tabnet_test_metrics['roc_auc'],\n",
        "        'test_pr_auc': tabnet_test_metrics['pr_auc']\n",
        "    })\n",
        "\n",
        "if model_comparison:\n",
        "    comparison_df = pd.DataFrame(model_comparison)\n",
        "    print(\"Model Comparison (Validation Set):\")\n",
        "    print(comparison_df[['model', 'val_roc_auc', 'val_pr_auc']].to_string(index=False))\n",
        "    \n",
        "    # Select best model based on validation PR-AUC\n",
        "    best_model_name = comparison_df.loc[comparison_df['val_pr_auc'].idxmax(), 'model']\n",
        "    print(f\"\\nBest model (by Val PR-AUC): {best_model_name}\")\n",
        "    \n",
        "    # Get best model\n",
        "    if best_model_name == 'LightGBM' and LIGHTGBM_AVAILABLE:\n",
        "        best_model = lgb_model\n",
        "    elif best_model_name == 'CatBoost' and CATBOOST_AVAILABLE:\n",
        "        best_model = cat_model\n",
        "    elif best_model_name == 'TabNet' and TABNET_AVAILABLE:\n",
        "        best_model = tabnet_model\n",
        "    else:\n",
        "        best_model = None\n",
        "else:\n",
        "    print(\"No models available for comparison\")\n",
        "    best_model = None\n",
        "    best_model_name = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Model Persistence\n",
        "\n",
        "Save the best model for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if best_model is not None:\n",
        "    import joblib\n",
        "    \n",
        "    # Save best model\n",
        "    model_output_path = MODELS_ROOT / \"best_podium_model.pkl\"\n",
        "    joblib.dump(best_model, model_output_path)\n",
        "    print(f\"Best model saved to: {model_output_path}\")\n",
        "    \n",
        "    # Save comparison results\n",
        "    if model_comparison:\n",
        "        comparison_df.to_csv(PROCESSED_ROOT / \"model_comparison.csv\", index=False)\n",
        "        print(f\"Model comparison saved to: {PROCESSED_ROOT / 'model_comparison.csv'}\")\n",
        "else:\n",
        "    print(\"No model to save\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Trained LightGBM, CatBoost, and TabNet models on 1994-2022 data\n",
        "- Validated on 2023 data, tested on 2024 data\n",
        "- Selected best model based on validation PR-AUC\n",
        "- Saved best model to `models/best_podium_model.pkl` for downstream inference\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
