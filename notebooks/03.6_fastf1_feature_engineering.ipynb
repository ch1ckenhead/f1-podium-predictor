{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03.6 - FastF1 Feature Engineering\n",
        "\n",
        "This notebook extracts features from FastF1 data (2018+) and appends them to `master_races_clean.csv`.\n",
        "\n",
        "**Features:**\n",
        "- **Relative features** (standardized within race): DRS patterns, overtaking skill, position change rate, lap time std dev, pit stop timing, sector speed vs lap time, tyre efficiency index\n",
        "- **Weather features**: AirTemp, TrackTemp, Humidity, Pressure, WindSpeed, Rainfall flags and transitions\n",
        "\n",
        "**Key Requirements:**\n",
        "- Features calculated from historical race data (rolling averages)\n",
        "- Relative features standardized per race using z-score for cross-track comparability\n",
        "- Features must be available before the race (Practice/Qualifying + historical data)\n",
        "- All features joined to master_races_clean.csv (from 03.8)\n",
        "\n",
        "**Input:** \n",
        "- `data/processed/master_races_clean.csv` (from 03.8)\n",
        "- `data/raw/fastf1_2018plus/ALL_LAPS_*.csv`\n",
        "- `data/raw/fastf1_2018plus/ALL_TELEMETRY_*.csv`\n",
        "- `data/raw/fastf1_2018plus/ALL_WEATHER_*.csv`\n",
        "\n",
        "**Output:** `data/processed/master_races_with_fastf1.csv` - Clean dataset with FastF1 features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17:46:24 |     INFO | PROJECT_ROOT: C:\\Users\\erikv\\Downloads\\F1\n",
            "17:46:24 |     INFO | PROCESSED_ROOT: C:\\Users\\erikv\\Downloads\\F1\\data\\processed\n",
            "17:46:24 |     INFO | RAW_ROOT: C:\\Users\\erikv\\Downloads\\F1\\data\\raw\\fastf1_2018plus\n",
            "17:46:24 |     INFO | Loading master_races_clean.csv...\n",
            "17:46:25 |     INFO | Master dataset: 12,358 rows (total), 2,979 rows (2018+)\n",
            "17:46:25 |     INFO | Date range: 2018-03-25 00:00:00 to 2024-12-08 00:00:00\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from scipy import stats\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)8s | %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set up paths robustly for both script and notebook usage.\n",
        "\n",
        "import os\n",
        "\n",
        "# Try typical environment variables from Jupyter/Colab, fall back to cwd.\n",
        "def find_project_root():\n",
        "    # 1. Check environment variable for absolute project root.\n",
        "    prj_env = os.environ.get('PROJECT_ROOT', None)\n",
        "    if prj_env and Path(prj_env).exists():\n",
        "        return Path(prj_env).resolve()\n",
        "    # 2. Check common notebook variable for repo root.\n",
        "    if 'PWD' in os.environ:  # Jupyter sets $PWD to the notebook's start dir\n",
        "        root = Path(os.environ['PWD']).resolve()\n",
        "        # If this is inside the project, try project root\n",
        "        # Navigate up if typical subdirectory present\n",
        "        for test_up in [root, root.parent, root.parent.parent]:\n",
        "            # Check if 'data/processed/master_races_clean.csv' exists there\n",
        "            if (test_up / \"data\" / \"processed\" / \"master_races_clean.csv\").exists():\n",
        "                return test_up\n",
        "    # 3. Try cwd and parents upwards\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for up in [cwd, *cwd.parents]:\n",
        "        if (up / \"data\" / \"processed\" / \"master_races_clean.csv\").exists():\n",
        "            return up\n",
        "    # 4. Fallback to relative parent (legacy; may be wrong on some setups)\n",
        "    fallback = Path(\"..\").resolve()\n",
        "    return fallback\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "\n",
        "PROCESSED_ROOT = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "RAW_ROOT = PROJECT_ROOT / \"data\" / \"raw\" / \"fastf1_2018plus\"\n",
        "\n",
        "# FastF1 data years (2018+)\n",
        "FASTF1_YEARS = range(2018, 2026)\n",
        "\n",
        "logger.info(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
        "logger.info(f\"PROCESSED_ROOT: {PROCESSED_ROOT}\")\n",
        "logger.info(f\"RAW_ROOT: {RAW_ROOT}\")\n",
        "\n",
        "# Load master dataset (from 03.8 - cleaned dataset)\n",
        "master_path = PROCESSED_ROOT / \"master_races_clean.csv\"\n",
        "if not master_path.exists():\n",
        "    raise FileNotFoundError(f\"master_races_clean.csv not found at {master_path}. Please run notebook 03.8 first.\")\n",
        "\n",
        "logger.info(\"Loading master_races_clean.csv...\")\n",
        "master = pd.read_csv(master_path, low_memory=False)\n",
        "master['date'] = pd.to_datetime(master['date'], errors='coerce')\n",
        "master = master.sort_values(['year', 'round', 'date']).reset_index(drop=True)\n",
        "\n",
        "# Filter to 2018+ for FastF1 data\n",
        "master_2018plus = master[master['year'] >= 2018].copy()\n",
        "\n",
        "# Initialize variables for later cells\n",
        "fastf1_data = {'LAPS': {}, 'TELEMETRY': {}, 'WEATHER': {}}\n",
        "historical_metrics = pd.DataFrame()\n",
        "metrics_with_rolling = pd.DataFrame()\n",
        "weather_features = pd.DataFrame()\n",
        "master_with_features = pd.DataFrame()\n",
        "\n",
        "logger.info(f\"Master dataset: {len(master):,} rows (total), {len(master_2018plus):,} rows (2018+)\")\n",
        "logger.info(f\"Date range: {master_2018plus['date'].min()} to {master_2018plus['date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load FastF1 Data and Enrich with raceId\n",
        "\n",
        "Load all FastF1 CSV files (LAPS, TELEMETRY, WEATHER) and enrich them with `raceId` from master CSV by matching Year and Event. This makes temporal ordering and rolling averages much easier since raceId is in chronological order.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17:46:25 |     INFO | Loading FastF1 data files...\n",
            "17:46:25 |     INFO | Loading LAPS 2018...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17:46:26 |     INFO |   LAPS 2018: 58,002 rows\n",
            "17:46:26 |     INFO | Loading WEATHER 2018...\n",
            "17:46:26 |     INFO |   WEATHER 2018: 9,707 rows\n",
            "17:46:26 |     INFO | Loading TELEMETRY 2018...\n",
            "17:46:26 |     INFO |   TELEMETRY 2018 is large (2.34 GB), loading in chunks...\n",
            "17:47:43 |     INFO |   TELEMETRY 2018: 19,140,560 rows\n",
            "17:47:43 |     INFO | Loading LAPS 2019...\n",
            "17:47:45 |     INFO |   LAPS 2019: 45,169 rows\n",
            "17:47:45 |     INFO | Loading WEATHER 2019...\n",
            "17:47:45 |     INFO |   WEATHER 2019: 8,077 rows\n",
            "17:47:45 |     INFO | Loading TELEMETRY 2019...\n",
            "17:47:45 |     INFO |   TELEMETRY 2019 is large (6.31 GB), loading in chunks...\n",
            "17:53:13 |     INFO |   TELEMETRY 2019: 51,628,140 rows\n",
            "17:53:13 |     INFO | Loading LAPS 2020...\n",
            "17:53:16 |     INFO |   LAPS 2020: 39,040 rows\n",
            "17:53:16 |     INFO | Loading WEATHER 2020...\n",
            "17:53:16 |     INFO |   WEATHER 2020: 7,822 rows\n",
            "17:53:16 |     INFO | Loading TELEMETRY 2020...\n",
            "17:53:16 |     INFO |   TELEMETRY 2020 is large (2.01 GB), loading in chunks...\n",
            "17:55:22 |     INFO |   TELEMETRY 2020: 16,355,180 rows\n",
            "17:55:22 |     INFO | Loading LAPS 2021...\n",
            "17:55:28 |     INFO |   LAPS 2021: 60,296 rows\n",
            "17:55:28 |     INFO | Loading WEATHER 2021...\n",
            "17:55:28 |     INFO |   WEATHER 2021: 10,616 rows\n",
            "17:55:29 |     INFO | Loading TELEMETRY 2021...\n",
            "17:55:29 |     INFO |   TELEMETRY 2021 is large (6.48 GB), loading in chunks...\n",
            "18:02:23 |     INFO |   TELEMETRY 2021: 52,824,160 rows\n",
            "18:02:23 |     INFO | Loading LAPS 2022...\n",
            "18:02:26 |     INFO |   LAPS 2022: 59,565 rows\n",
            "18:02:26 |     INFO | Loading WEATHER 2022...\n",
            "18:02:26 |     INFO |   WEATHER 2022: 11,285 rows\n",
            "18:02:26 |     INFO | Loading TELEMETRY 2022...\n",
            "18:02:40 |     INFO |   TELEMETRY 2022: 1,725,796 rows\n",
            "18:02:40 |     INFO | Loading LAPS 2023...\n",
            "18:02:42 |     INFO |   LAPS 2023: 57,491 rows\n",
            "18:02:42 |     INFO | Loading WEATHER 2023...\n",
            "18:02:42 |     INFO |   WEATHER 2023: 10,613 rows\n",
            "18:02:42 |     INFO | Loading TELEMETRY 2023...\n",
            "18:02:42 |     INFO |   TELEMETRY 2023 is large (5.77 GB), loading in chunks...\n"
          ]
        }
      ],
      "source": [
        "def normalize_event_name(name: str) -> str:\n",
        "    \"\"\"Normalize event name for matching between FastF1 and master CSV.\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return None\n",
        "    # Convert to string and strip\n",
        "    name = str(name).strip()\n",
        "    return name\n",
        "\n",
        "def enrich_fastf1_with_raceid(fastf1_data: Dict, master_df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"Enrich FastF1 data with raceId from master CSV by matching Year and Event.\n",
        "    \n",
        "    This makes temporal ordering and historical calculations much easier.\n",
        "    \"\"\"\n",
        "    logger.info(\"Enriching FastF1 data with raceId from master CSV...\")\n",
        "    \n",
        "    # Create a mapping from (year, event) -> raceId\n",
        "    # Use the first raceId for each year+event combination (since one race = one raceId)\n",
        "    race_mapping = master_df[['year', 'name', 'raceId']].drop_duplicates(['year', 'name'])\n",
        "    race_mapping_dict = {}\n",
        "    for _, row in race_mapping.iterrows():\n",
        "        key = (int(row['year']), normalize_event_name(row['name']))\n",
        "        race_mapping_dict[key] = int(row['raceId'])\n",
        "    \n",
        "    logger.info(f\"Created raceId mapping for {len(race_mapping_dict)} unique race events\")\n",
        "    \n",
        "    # Enrich each dataset\n",
        "    enriched_data = {\n",
        "        'LAPS': {},\n",
        "        'TELEMETRY': {},\n",
        "        'WEATHER': {}\n",
        "    }\n",
        "    \n",
        "    for dataset_type in ['LAPS', 'TELEMETRY', 'WEATHER']:\n",
        "        for year in fastf1_data[dataset_type].keys():\n",
        "            df = fastf1_data[dataset_type][year].copy()\n",
        "            \n",
        "            # Add raceId by matching Year and Event\n",
        "            def get_raceid(row):\n",
        "                event = normalize_event_name(row['Event']) if 'Event' in row.index else None\n",
        "                year_val = int(row['Year']) if 'Year' in row.index else None\n",
        "                if event and year_val:\n",
        "                    key = (year_val, event)\n",
        "                    return race_mapping_dict.get(key, None)\n",
        "                return None\n",
        "            \n",
        "            # Apply raceId mapping\n",
        "            df['raceId'] = df.apply(get_raceid, axis=1)\n",
        "            \n",
        "            # Log statistics\n",
        "            total_rows = len(df)\n",
        "            matched_rows = df['raceId'].notna().sum()\n",
        "            match_rate = (matched_rows / total_rows * 100) if total_rows > 0 else 0\n",
        "            \n",
        "            logger.info(f\"  {dataset_type} {year}: {matched_rows:,}/{total_rows:,} rows matched ({match_rate:.1f}%)\")\n",
        "            \n",
        "            enriched_data[dataset_type][year] = df\n",
        "    \n",
        "    return enriched_data\n",
        "\n",
        "def load_fastf1_data(years: range, raw_root: Path) -> Dict[str, Dict[int, pd.DataFrame]]:\n",
        "    \"\"\"Load all FastF1 CSV files for specified years.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with keys: 'LAPS', 'TELEMETRY', 'WEATHER'\n",
        "        Each contains a dict mapping year -> DataFrame\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'LAPS': {},\n",
        "        'TELEMETRY': {},\n",
        "        'WEATHER': {}\n",
        "    }\n",
        "    \n",
        "    for year in years:\n",
        "        # Load LAPS\n",
        "        laps_path = raw_root / f\"ALL_LAPS_{year}.csv\"\n",
        "        if laps_path.exists():\n",
        "            try:\n",
        "                logger.info(f\"Loading LAPS {year}...\")\n",
        "                df = pd.read_csv(laps_path, low_memory=False)\n",
        "                if len(df) > 0:\n",
        "                    # Convert LapTime to timedelta for easier calculations\n",
        "                    if 'LapTime' in df.columns:\n",
        "                        df['LapTime'] = pd.to_timedelta(df['LapTime'], errors='coerce')\n",
        "                    data['LAPS'][year] = df\n",
        "                    logger.info(f\"  LAPS {year}: {len(df):,} rows\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"  Failed to load LAPS {year}: {e}\")\n",
        "        \n",
        "        # Load WEATHER\n",
        "        weather_path = raw_root / f\"ALL_WEATHER_{year}.csv\"\n",
        "        if weather_path.exists():\n",
        "            try:\n",
        "                logger.info(f\"Loading WEATHER {year}...\")\n",
        "                df = pd.read_csv(weather_path, low_memory=False)\n",
        "                if len(df) > 0:\n",
        "                    data['WEATHER'][year] = df\n",
        "                    logger.info(f\"  WEATHER {year}: {len(df):,} rows\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"  Failed to load WEATHER {year}: {e}\")\n",
        "        \n",
        "        # Load TELEMETRY (in chunks if very large)\n",
        "        telemetry_path = raw_root / f\"ALL_TELEMETRY_{year}.csv\"\n",
        "        if telemetry_path.exists():\n",
        "            try:\n",
        "                logger.info(f\"Loading TELEMETRY {year}...\")\n",
        "                # Check file size first\n",
        "                file_size = telemetry_path.stat().st_size / (1024**3)  # GB\n",
        "                if file_size > 1.0:  # > 1GB, load in chunks\n",
        "                    logger.info(f\"  TELEMETRY {year} is large ({file_size:.2f} GB), loading in chunks...\")\n",
        "                    chunks = []\n",
        "                    chunk_size = 100000\n",
        "                    for chunk in pd.read_csv(telemetry_path, chunksize=chunk_size, low_memory=False):\n",
        "                        chunks.append(chunk)\n",
        "                    df = pd.concat(chunks, ignore_index=True)\n",
        "                else:\n",
        "                    df = pd.read_csv(telemetry_path, low_memory=False)\n",
        "                \n",
        "                if len(df) > 0:\n",
        "                    data['TELEMETRY'][year] = df\n",
        "                    logger.info(f\"  TELEMETRY {year}: {len(df):,} rows\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"  Failed to load TELEMETRY {year}: {e}\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Load FastF1 data\n",
        "logger.info(\"Loading FastF1 data files...\")\n",
        "fastf1_data = load_fastf1_data(FASTF1_YEARS, RAW_ROOT)\n",
        "\n",
        "# Summary\n",
        "for dataset_type in ['LAPS', 'TELEMETRY', 'WEATHER']:\n",
        "    years_loaded = list(fastf1_data[dataset_type].keys())\n",
        "    logger.info(f\"{dataset_type}: {len(years_loaded)} years loaded: {years_loaded}\")\n",
        "\n",
        "logger.info(\"FastF1 data loading complete!\")\n",
        "\n",
        "# Enrich FastF1 data with raceId from master CSV\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"Enriching FastF1 data with raceId...\")\n",
        "fastf1_data = enrich_fastf1_with_raceid(fastf1_data, master_2018plus)\n",
        "logger.info(\"FastF1 data enrichment complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Calculate Historical Race Metrics\n",
        "\n",
        "Calculate base metrics from historical race data (Session='R') for each driver-race combination:\n",
        "- DRS patterns (from TELEMETRY)\n",
        "- Overtaking events (position changes)\n",
        "- Lap time statistics (mean, std dev per stint)\n",
        "- Pit stop timings (first pit lap)\n",
        "- Sector speed correlations\n",
        "- Tyre efficiency metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "03:18:45 |     INFO | Historical race metrics calculation functions defined.\n"
          ]
        }
      ],
      "source": [
        "def calculate_drs_patterns(telemetry_df: pd.DataFrame, driver_code: str, driver_number: int, \n",
        "                           year: int, event: str, race_id: Optional[int] = None) -> Dict[str, float]:\n",
        "    \"\"\"Calculate DRS usage patterns for a driver in a race.\"\"\"\n",
        "    # Filter telemetry for this driver/race\n",
        "    # Telemetry Driver column can be driver number (int) or driver code (str)\n",
        "    if 'Driver' not in telemetry_df.columns:\n",
        "        return {'drs_activation_rate': np.nan, 'drs_time_fraction': np.nan}\n",
        "    \n",
        "    # Use raceId if available for more efficient matching\n",
        "    if 'raceId' in telemetry_df.columns and race_id is not None:\n",
        "        race_telemetry = telemetry_df[\n",
        "            (telemetry_df['raceId'] == race_id) &\n",
        "            (telemetry_df['Session'] == 'R')\n",
        "        ].copy()\n",
        "    else:\n",
        "        # Fallback to Year + Event matching\n",
        "        race_telemetry = telemetry_df[\n",
        "            (telemetry_df['Year'] == year) &\n",
        "            (telemetry_df['Event'] == event) &\n",
        "            (telemetry_df['Session'] == 'R')\n",
        "        ].copy()\n",
        "    \n",
        "    # Match on driver number (telemetry usually has numeric driver IDs)\n",
        "    if len(race_telemetry) > 0:\n",
        "        if race_telemetry['Driver'].dtype in [np.int64, np.float64]:\n",
        "            race_telemetry = race_telemetry[race_telemetry['Driver'] == driver_number]\n",
        "        else:\n",
        "            # Try matching on driver code as string\n",
        "            race_telemetry = race_telemetry[race_telemetry['Driver'].astype(str) == str(driver_code)]\n",
        "    \n",
        "    if len(race_telemetry) == 0 or 'DRS' not in race_telemetry.columns:\n",
        "        return {'drs_activation_rate': np.nan, 'drs_time_fraction': np.nan}\n",
        "    \n",
        "    # DRS activation rate (percentage of samples with DRS=1)\n",
        "    drs_activation_rate = (race_telemetry['DRS'] == 1).mean() if 'DRS' in race_telemetry.columns else np.nan\n",
        "    \n",
        "    # DRS time fraction (if we have time data)\n",
        "    drs_time_fraction = drs_activation_rate  # Simplified - could be more sophisticated\n",
        "    \n",
        "    return {\n",
        "        'drs_activation_rate': drs_activation_rate,\n",
        "        'drs_time_fraction': drs_time_fraction\n",
        "    }\n",
        "\n",
        "def calculate_overtaking_skill(laps_df: pd.DataFrame, driver_code: str, driver_number: int,\n",
        "                               year: int, event: str) -> Dict[str, float]:\n",
        "    \"\"\"Calculate overtaking skill metrics (position changes).\"\"\"\n",
        "    # Filter laps for this driver/race\n",
        "    race_laps = laps_df[\n",
        "        (laps_df['Year'] == year) &\n",
        "        (laps_df['Event'] == event) &\n",
        "        (laps_df['Session'] == 'R') &\n",
        "        (laps_df['Driver'] == driver_code) &\n",
        "        (laps_df['DriverNumber'] == driver_number) &\n",
        "        (laps_df['Position'].notna())\n",
        "    ].copy()\n",
        "    \n",
        "    if len(race_laps) < 2:\n",
        "        return {'position_changes': np.nan, 'position_change_rate': np.nan}\n",
        "    \n",
        "    race_laps = race_laps.sort_values('LapNumber')\n",
        "    \n",
        "    # Calculate position changes\n",
        "    position_changes = (race_laps['Position'].diff() != 0).sum() - 1  # Subtract 1 for first lap\n",
        "    position_changes = max(0, position_changes)  # Can't be negative\n",
        "    \n",
        "    # Position change rate (changes per lap)\n",
        "    total_laps = len(race_laps)\n",
        "    position_change_rate = position_changes / total_laps if total_laps > 0 else np.nan\n",
        "    \n",
        "    return {\n",
        "        'position_changes': position_changes,\n",
        "        'position_change_rate': position_change_rate\n",
        "    }\n",
        "\n",
        "def calculate_lap_time_stats(laps_df: pd.DataFrame, driver_code: str, driver_number: int,\n",
        "                            year: int, event: str) -> Dict[str, float]:\n",
        "    \"\"\"Calculate lap time statistics (mean, std dev) per stint.\"\"\"\n",
        "    # Filter laps for this driver/race\n",
        "    race_laps = laps_df[\n",
        "        (laps_df['Year'] == year) &\n",
        "        (laps_df['Event'] == event) &\n",
        "        (laps_df['Session'] == 'R') &\n",
        "        (laps_df['Driver'] == driver_code) &\n",
        "        (laps_df['DriverNumber'] == driver_number) &\n",
        "        (laps_df['LapTime'].notna()) &\n",
        "        (laps_df['Deleted'] != True)  # Exclude deleted laps\n",
        "    ].copy()\n",
        "    \n",
        "    if len(race_laps) == 0:\n",
        "        return {'lap_time_mean': np.nan, 'lap_time_std': np.nan}\n",
        "    \n",
        "    # Convert LapTime to seconds if it's timedelta\n",
        "    if pd.api.types.is_timedelta64_dtype(race_laps['LapTime']):\n",
        "        lap_times_seconds = race_laps['LapTime'].dt.total_seconds()\n",
        "    else:\n",
        "        lap_times_seconds = pd.to_timedelta(race_laps['LapTime'], errors='coerce').dt.total_seconds()\n",
        "    \n",
        "    lap_time_mean = lap_times_seconds.mean()\n",
        "    lap_time_std = lap_times_seconds.std()\n",
        "    \n",
        "    return {\n",
        "        'lap_time_mean': lap_time_mean,\n",
        "        'lap_time_std': lap_time_std\n",
        "    }\n",
        "\n",
        "def calculate_pit_stop_timing(laps_df: pd.DataFrame, driver_code: str, driver_number: int,\n",
        "                             year: int, event: str) -> Dict[str, float]:\n",
        "    \"\"\"Calculate pit stop timing (lap number of first pit stop).\"\"\"\n",
        "    # Filter laps for this driver/race\n",
        "    race_laps = laps_df[\n",
        "        (laps_df['Year'] == year) &\n",
        "        (laps_df['Event'] == event) &\n",
        "        (laps_df['Session'] == 'R') &\n",
        "        (laps_df['Driver'] == driver_code) &\n",
        "        (laps_df['DriverNumber'] == driver_number) &\n",
        "        (laps_df['LapNumber'].notna())\n",
        "    ].copy()\n",
        "    \n",
        "    if len(race_laps) == 0:\n",
        "        return {'first_pit_lap': np.nan, 'pit_stops': 0}\n",
        "    \n",
        "    race_laps = race_laps.sort_values('LapNumber')\n",
        "    \n",
        "    # Check for pit stops (Stint changes or PitInTime/PitOutTime)\n",
        "    if 'Stint' in race_laps.columns:\n",
        "        stint_changes = race_laps[race_laps['Stint'].diff() > 0]\n",
        "        if len(stint_changes) > 0:\n",
        "            first_pit_lap = stint_changes.iloc[0]['LapNumber']\n",
        "            pit_stops = len(stint_changes)\n",
        "        else:\n",
        "            first_pit_lap = np.nan\n",
        "            pit_stops = 0\n",
        "    else:\n",
        "        # Alternative: check PitInTime\n",
        "        pit_laps = race_laps[race_laps['PitInTime'].notna()]\n",
        "        if len(pit_laps) > 0:\n",
        "            first_pit_lap = pit_laps.iloc[0]['LapNumber']\n",
        "            pit_stops = len(pit_laps)\n",
        "        else:\n",
        "            first_pit_lap = np.nan\n",
        "            pit_stops = 0\n",
        "    \n",
        "    return {\n",
        "        'first_pit_lap': first_pit_lap,\n",
        "        'pit_stops': pit_stops\n",
        "    }\n",
        "\n",
        "def calculate_sector_speed_correlation(laps_df: pd.DataFrame, driver_code: str, driver_number: int,\n",
        "                                      year: int, event: str) -> Dict[str, float]:\n",
        "    \"\"\"Calculate correlation between sector speeds and lap time.\"\"\"\n",
        "    # Filter laps for this driver/race\n",
        "    race_laps = laps_df[\n",
        "        (laps_df['Year'] == year) &\n",
        "        (laps_df['Event'] == event) &\n",
        "        (laps_df['Session'] == 'R') &\n",
        "        (laps_df['Driver'] == driver_code) &\n",
        "        (laps_df['DriverNumber'] == driver_number) &\n",
        "        (laps_df['LapTime'].notna()) &\n",
        "        (laps_df['Deleted'] != True)\n",
        "    ].copy()\n",
        "    \n",
        "    if len(race_laps) < 3:\n",
        "        return {'sector_speed_laptime_corr': np.nan}\n",
        "    \n",
        "    # Convert LapTime to seconds\n",
        "    if pd.api.types.is_timedelta64_dtype(race_laps['LapTime']):\n",
        "        lap_times_seconds = race_laps['LapTime'].dt.total_seconds()\n",
        "    else:\n",
        "        lap_times_seconds = pd.to_timedelta(race_laps['LapTime'], errors='coerce').dt.total_seconds()\n",
        "    \n",
        "    # Calculate average sector speed (using SpeedI1 and SpeedI2)\n",
        "    if 'SpeedI1' in race_laps.columns and 'SpeedI2' in race_laps.columns:\n",
        "        sector_speeds = (race_laps['SpeedI1'].fillna(0) + race_laps['SpeedI2'].fillna(0)) / 2\n",
        "        sector_speeds = sector_speeds.replace(0, np.nan)  # Replace 0 with NaN\n",
        "        \n",
        "        # Calculate correlation\n",
        "        valid_data = pd.DataFrame({\n",
        "            'speed': sector_speeds,\n",
        "            'lap_time': lap_times_seconds\n",
        "        }).dropna()\n",
        "        \n",
        "        if len(valid_data) >= 3:\n",
        "            corr = valid_data['speed'].corr(valid_data['lap_time'])\n",
        "            # Invert correlation (higher speed should correlate with lower lap time)\n",
        "            # So we use -corr or (1/corr) for efficiency metric\n",
        "            sector_speed_laptime_corr = -corr if not pd.isna(corr) else np.nan\n",
        "        else:\n",
        "            sector_speed_laptime_corr = np.nan\n",
        "    else:\n",
        "        sector_speed_laptime_corr = np.nan\n",
        "    \n",
        "    return {\n",
        "        'sector_speed_laptime_corr': sector_speed_laptime_corr\n",
        "    }\n",
        "\n",
        "def calculate_tyre_efficiency(laps_df: pd.DataFrame, driver_code: str, driver_number: int,\n",
        "                             year: int, event: str) -> Dict[str, float]:\n",
        "    \"\"\"Calculate tyre efficiency index (average lap time per compound normalized by tyre life).\"\"\"\n",
        "    # Filter laps for this driver/race\n",
        "    race_laps = laps_df[\n",
        "        (laps_df['Year'] == year) &\n",
        "        (laps_df['Event'] == event) &\n",
        "        (laps_df['Session'] == 'R') &\n",
        "        (laps_df['Driver'] == driver_code) &\n",
        "        (laps_df['DriverNumber'] == driver_number) &\n",
        "        (laps_df['LapTime'].notna()) &\n",
        "        (laps_df['Compound'].notna()) &\n",
        "        (laps_df['Deleted'] != True)\n",
        "    ].copy()\n",
        "    \n",
        "    if len(race_laps) == 0 or 'Compound' not in race_laps.columns:\n",
        "        return {'tyre_efficiency_index': np.nan}\n",
        "    \n",
        "    # Convert LapTime to seconds\n",
        "    if pd.api.types.is_timedelta64_dtype(race_laps['LapTime']):\n",
        "        lap_times_seconds = race_laps['LapTime'].dt.total_seconds()\n",
        "    else:\n",
        "        lap_times_seconds = pd.to_timedelta(race_laps['LapTime'], errors='coerce').dt.total_seconds()\n",
        "    \n",
        "    race_laps['LapTimeSeconds'] = lap_times_seconds\n",
        "    \n",
        "    # Group by stint/compound and calculate efficiency\n",
        "    if 'Stint' in race_laps.columns and 'TyreLife' in race_laps.columns:\n",
        "        # Calculate per stint\n",
        "        stints = race_laps.groupby('Stint')\n",
        "        efficiencies = []\n",
        "        \n",
        "        for stint_num, stint_data in stints:\n",
        "            if len(stint_data) < 2:\n",
        "                continue\n",
        "            \n",
        "            avg_lap_time = stint_data['LapTimeSeconds'].mean()\n",
        "            max_tyre_life = stint_data['TyreLife'].max()\n",
        "            \n",
        "            if max_tyre_life > 0 and not pd.isna(avg_lap_time):\n",
        "                # Efficiency = average lap time / tyre life (lower is better)\n",
        "                efficiency = avg_lap_time / max_tyre_life\n",
        "                efficiencies.append(efficiency)\n",
        "        \n",
        "        if len(efficiencies) > 0:\n",
        "            tyre_efficiency_index = np.mean(efficiencies)\n",
        "        else:\n",
        "            tyre_efficiency_index = np.nan\n",
        "    else:\n",
        "        # Simplified: overall average\n",
        "        if len(race_laps) > 0:\n",
        "            avg_lap_time = race_laps['LapTimeSeconds'].mean()\n",
        "            tyre_efficiency_index = avg_lap_time  # Simplified metric\n",
        "        else:\n",
        "            tyre_efficiency_index = np.nan\n",
        "    \n",
        "    return {\n",
        "        'tyre_efficiency_index': tyre_efficiency_index\n",
        "    }\n",
        "\n",
        "logger.info(\"Historical race metrics calculation functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "03:18:45 |     INFO | Starting historical metrics calculation...\n",
            "03:18:45 |     INFO | Calculating historical metrics for 2,979 race-driver combinations...\n",
            "03:18:57 |     INFO | Combined LAPS data: 403,492 rows\n",
            "03:18:57 |     INFO | Combined TELEMETRY data: 29,934,157 rows\n",
            "03:18:57 |     INFO | Combined WEATHER data: 72,836 rows\n",
            "03:21:30 |     INFO | Processed 100/2979 race-driver combinations...\n",
            "03:23:50 |     INFO | Processed 200/2979 race-driver combinations...\n",
            "03:26:24 |     INFO | Processed 300/2979 race-driver combinations...\n",
            "03:29:10 |     INFO | Processed 400/2979 race-driver combinations...\n",
            "03:32:07 |     INFO | Processed 500/2979 race-driver combinations...\n",
            "03:34:57 |     INFO | Processed 600/2979 race-driver combinations...\n",
            "03:37:53 |     INFO | Processed 700/2979 race-driver combinations...\n",
            "03:40:33 |     INFO | Processed 800/2979 race-driver combinations...\n",
            "03:42:59 |     INFO | Processed 900/2979 race-driver combinations...\n",
            "03:45:14 |     INFO | Processed 1000/2979 race-driver combinations...\n",
            "03:47:28 |     INFO | Processed 1100/2979 race-driver combinations...\n",
            "03:49:43 |     INFO | Processed 1200/2979 race-driver combinations...\n",
            "03:51:57 |     INFO | Processed 1300/2979 race-driver combinations...\n",
            "03:54:10 |     INFO | Processed 1400/2979 race-driver combinations...\n",
            "03:56:22 |     INFO | Processed 1500/2979 race-driver combinations...\n",
            "03:58:36 |     INFO | Processed 1600/2979 race-driver combinations...\n",
            "04:01:02 |     INFO | Processed 1700/2979 race-driver combinations...\n",
            "04:03:34 |     INFO | Processed 1800/2979 race-driver combinations...\n",
            "04:06:03 |     INFO | Processed 1900/2979 race-driver combinations...\n",
            "04:08:17 |     INFO | Processed 2000/2979 race-driver combinations...\n",
            "04:10:30 |     INFO | Processed 2100/2979 race-driver combinations...\n",
            "04:12:43 |     INFO | Processed 2200/2979 race-driver combinations...\n",
            "04:14:57 |     INFO | Processed 2300/2979 race-driver combinations...\n",
            "04:17:10 |     INFO | Processed 2400/2979 race-driver combinations...\n",
            "04:19:24 |     INFO | Processed 2500/2979 race-driver combinations...\n",
            "04:21:37 |     INFO | Processed 2600/2979 race-driver combinations...\n",
            "04:23:50 |     INFO | Processed 2700/2979 race-driver combinations...\n",
            "04:26:04 |     INFO | Processed 2800/2979 race-driver combinations...\n",
            "04:28:17 |     INFO | Processed 2900/2979 race-driver combinations...\n",
            "04:30:02 |     INFO | Completed calculation for 2,979 race-driver combinations\n",
            "04:30:06 |     INFO | Historical metrics calculated: (2979, 17)\n",
            "04:30:06 |     INFO | Columns: ['year', 'event', 'driver_code', 'driver_number', 'race_date', 'race_id', 'driver_id', 'position_changes', 'position_change_rate', 'lap_time_mean', 'lap_time_std', 'first_pit_lap', 'pit_stops', 'sector_speed_laptime_corr', 'tyre_efficiency_index', 'drs_activation_rate', 'drs_time_fraction']\n",
            "04:30:06 |     INFO | Sample metrics:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   year                  event driver_code  driver_number  race_date  race_id  \\\n",
            "0  2018  Australian Grand Prix         VET              5 2018-03-25      989   \n",
            "1  2018  Australian Grand Prix         HAM             44 2018-03-25      989   \n",
            "2  2018  Australian Grand Prix         RAI              7 2018-03-25      989   \n",
            "3  2018  Australian Grand Prix         RIC              3 2018-03-25      989   \n",
            "4  2018  Australian Grand Prix         ALO             14 2018-03-25      989   \n",
            "\n",
            "   driver_id  position_changes  position_change_rate  lap_time_mean  \\\n",
            "0         20               2.0              0.034483      92.642810   \n",
            "1          1               1.0              0.017241      92.729638   \n",
            "2          8               1.0              0.017241      92.751586   \n",
            "3        817               4.0              0.068966      92.764690   \n",
            "4          4               5.0              0.086207      93.123603   \n",
            "\n",
            "   lap_time_std  first_pit_lap  pit_stops  sector_speed_laptime_corr  \\\n",
            "0     13.366049           27.0          1                   0.421660   \n",
            "1     13.008674           20.0          1                   0.600831   \n",
            "2     12.714170           19.0          1                   0.343853   \n",
            "3     12.157700           27.0          1                   0.384370   \n",
            "4     11.767294           27.0          1                   0.392931   \n",
            "\n",
            "   tyre_efficiency_index  drs_activation_rate  drs_time_fraction  \n",
            "0               3.088294                  NaN                NaN  \n",
            "1               3.450398                  NaN                NaN  \n",
            "2               3.425301                  NaN                NaN  \n",
            "3               3.098265                  NaN                NaN  \n",
            "4               3.307610                  NaN                NaN  \n"
          ]
        }
      ],
      "source": [
        "def calculate_all_historical_metrics(fastf1_data: Dict, master_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculate all historical metrics for each race-driver combination.\n",
        "    \n",
        "    Returns DataFrame with one row per (year, event, driver_code, driver_number) \n",
        "    with all calculated metrics.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Get unique race-driver combinations from master CSV (2018+)\n",
        "    races = master_df[master_df['year'] >= 2018].copy()\n",
        "    \n",
        "    logger.info(f\"Calculating historical metrics for {len(races):,} race-driver combinations...\")\n",
        "    \n",
        "    # Combine all years of data into single DataFrames for efficiency\n",
        "    all_laps = []\n",
        "    all_telemetry = []\n",
        "    all_weather = []\n",
        "    \n",
        "    for year in sorted(fastf1_data['LAPS'].keys()):\n",
        "        if year in fastf1_data['LAPS']:\n",
        "            df = fastf1_data['LAPS'][year].copy()\n",
        "            all_laps.append(df)\n",
        "    \n",
        "    for year in sorted(fastf1_data['TELEMETRY'].keys()):\n",
        "        if year in fastf1_data['TELEMETRY']:\n",
        "            df = fastf1_data['TELEMETRY'][year].copy()\n",
        "            all_telemetry.append(df)\n",
        "    \n",
        "    for year in sorted(fastf1_data['WEATHER'].keys()):\n",
        "        if year in fastf1_data['WEATHER']:\n",
        "            df = fastf1_data['WEATHER'][year].copy()\n",
        "            all_weather.append(df)\n",
        "    \n",
        "    if all_laps:\n",
        "        combined_laps = pd.concat(all_laps, ignore_index=True)\n",
        "        logger.info(f\"Combined LAPS data: {len(combined_laps):,} rows\")\n",
        "    else:\n",
        "        combined_laps = pd.DataFrame()\n",
        "        logger.warning(\"No LAPS data available!\")\n",
        "    \n",
        "    if all_telemetry:\n",
        "        combined_telemetry = pd.concat(all_telemetry, ignore_index=True)\n",
        "        logger.info(f\"Combined TELEMETRY data: {len(combined_telemetry):,} rows\")\n",
        "    else:\n",
        "        combined_telemetry = pd.DataFrame()\n",
        "        logger.warning(\"No TELEMETRY data available!\")\n",
        "    \n",
        "    if all_weather:\n",
        "        combined_weather = pd.concat(all_weather, ignore_index=True)\n",
        "        logger.info(f\"Combined WEATHER data: {len(combined_weather):,} rows\")\n",
        "    else:\n",
        "        combined_weather = pd.DataFrame()\n",
        "        logger.warning(\"No WEATHER data available!\")\n",
        "    \n",
        "    # Process each race-driver combination\n",
        "    processed = 0\n",
        "    total = len(races)\n",
        "    \n",
        "    for idx, row in races.iterrows():\n",
        "        year = int(row['year'])\n",
        "        event = normalize_event_name(row['name'])\n",
        "        driver_code = row['code']\n",
        "        driver_number = row['number_driver']\n",
        "        race_id = row.get('raceId', None)\n",
        "        \n",
        "        if pd.isna(event) or pd.isna(driver_code) or pd.isna(driver_number):\n",
        "            continue\n",
        "        \n",
        "        # Initialize result dict\n",
        "        result = {\n",
        "            'year': year,\n",
        "            'event': event,\n",
        "            'driver_code': driver_code,\n",
        "            'driver_number': int(driver_number) if not pd.isna(driver_number) else None,\n",
        "            'race_date': row['date'],\n",
        "            'race_id': race_id,\n",
        "            'driver_id': row.get('driverId', None),\n",
        "        }\n",
        "        \n",
        "        # Calculate metrics from LAPS data (race sessions only)\n",
        "        # Use raceId if available for more efficient matching\n",
        "        if len(combined_laps) > 0:\n",
        "            if 'raceId' in combined_laps.columns and race_id is not None:\n",
        "                # Use raceId for matching (faster and more reliable)\n",
        "                race_laps = combined_laps[\n",
        "                    (combined_laps['raceId'] == race_id) &\n",
        "                    (combined_laps['Session'] == 'R')\n",
        "                ]\n",
        "            else:\n",
        "                # Fallback to Year + Event matching\n",
        "                race_laps = combined_laps[\n",
        "                    (combined_laps['Year'] == year) &\n",
        "                    (combined_laps['Event'] == event) &\n",
        "                    (combined_laps['Session'] == 'R')\n",
        "                ]\n",
        "            \n",
        "            if len(race_laps) > 0:\n",
        "                # Overtaking skill\n",
        "                overtaking = calculate_overtaking_skill(race_laps, driver_code, int(driver_number), year, event)\n",
        "                result.update(overtaking)\n",
        "                \n",
        "                # Lap time stats\n",
        "                lap_stats = calculate_lap_time_stats(race_laps, driver_code, int(driver_number), year, event)\n",
        "                result.update(lap_stats)\n",
        "                \n",
        "                # Pit stop timing\n",
        "                pit_timing = calculate_pit_stop_timing(race_laps, driver_code, int(driver_number), year, event)\n",
        "                result.update(pit_timing)\n",
        "                \n",
        "                # Sector speed correlation\n",
        "                sector_corr = calculate_sector_speed_correlation(race_laps, driver_code, int(driver_number), year, event)\n",
        "                result.update(sector_corr)\n",
        "                \n",
        "                # Tyre efficiency\n",
        "                tyre_eff = calculate_tyre_efficiency(race_laps, driver_code, int(driver_number), year, event)\n",
        "                result.update(tyre_eff)\n",
        "        \n",
        "        # Calculate DRS patterns from TELEMETRY\n",
        "        if len(combined_telemetry) > 0:\n",
        "            drs_patterns = calculate_drs_patterns(combined_telemetry, driver_code, int(driver_number), year, event, race_id)\n",
        "            result.update(drs_patterns)\n",
        "        \n",
        "        results.append(result)\n",
        "        processed += 1\n",
        "        \n",
        "        if processed % 100 == 0:\n",
        "            logger.info(f\"Processed {processed}/{total} race-driver combinations...\")\n",
        "    \n",
        "    logger.info(f\"Completed calculation for {len(results):,} race-driver combinations\")\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    metrics_df = pd.DataFrame(results)\n",
        "    \n",
        "    return metrics_df\n",
        "\n",
        "# Calculate all historical metrics\n",
        "logger.info(\"Starting historical metrics calculation...\")\n",
        "if len(fastf1_data['LAPS']) > 0 or len(fastf1_data['TELEMETRY']) > 0:\n",
        "    historical_metrics = calculate_all_historical_metrics(fastf1_data, master_2018plus)\n",
        "    \n",
        "    logger.info(f\"Historical metrics calculated: {historical_metrics.shape}\")\n",
        "    logger.info(f\"Columns: {list(historical_metrics.columns)}\")\n",
        "    logger.info(f\"Sample metrics:\")\n",
        "    print(historical_metrics.head())\n",
        "else:\n",
        "    logger.warning(\"No FastF1 data available to calculate metrics!\")\n",
        "    historical_metrics = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10:18:56 |     INFO | Calculating rolling averages and standardizing features...\n",
            "10:18:56 |     INFO | Standardizing relative features per race...\n",
            "10:18:57 |     INFO | Calculating rolling averages from previous races only (using raceId for ordering)...\n",
            "10:18:57 |     INFO | Rolling averages and standardization complete. Shape: (2979, 47)\n",
            "10:18:57 |     INFO | Metrics with rolling averages: (2979, 47)\n",
            "10:18:57 |     INFO | New columns: ['drs_activation_rate_relative', 'position_change_rate_relative', 'lap_time_std_relative', 'first_pit_lap_relative', 'sector_speed_laptime_corr_relative', 'tyre_efficiency_index_relative', 'drs_activation_rate_avg_last_3', 'drs_activation_rate_avg_last_5', 'drs_activation_rate_avg_last_10', 'position_change_rate_avg_last_3']\n"
          ]
        }
      ],
      "source": [
        "def calculate_rolling_averages_and_standardize(metrics_df: pd.DataFrame, master_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculate rolling averages for each driver and standardize relative features per race.\n",
        "    \n",
        "    IMPORTANT: Rolling averages use ONLY previous races (shifted by 1) for predictive features.\n",
        "    Uses raceId for proper temporal ordering (raceId is in chronological order).\n",
        "    \n",
        "    Relative features to standardize:\n",
        "    - drs_activation_rate\n",
        "    - position_change_rate  \n",
        "    - lap_time_std\n",
        "    - first_pit_lap\n",
        "    - sector_speed_laptime_corr\n",
        "    - tyre_efficiency_index\n",
        "    \n",
        "    Returns DataFrame with rolling averages and standardized features.\n",
        "    \"\"\"\n",
        "    logger.info(\"Calculating rolling averages and standardizing features...\")\n",
        "    \n",
        "    # Prepare master_df - ensure number_driver is numeric for merging\n",
        "    master_df_prep = master_df[['year', 'name', 'code', 'number_driver', 'date', 'raceId', 'driverId']].copy()\n",
        "    \n",
        "    # Convert number_driver to numeric (handles both int and string representations)\n",
        "    master_df_prep['number_driver'] = pd.to_numeric(master_df_prep['number_driver'], errors='coerce')\n",
        "    \n",
        "    # Also ensure metrics_df driver_number is numeric\n",
        "    metrics_df_prep = metrics_df.copy()\n",
        "    if 'driver_number' in metrics_df_prep.columns:\n",
        "        metrics_df_prep['driver_number'] = pd.to_numeric(metrics_df_prep['driver_number'], errors='coerce')\n",
        "    \n",
        "    # Merge with master to get date and ensure we have raceId and driverId\n",
        "    metrics_with_master = metrics_df_prep.merge(\n",
        "        master_df_prep,\n",
        "        left_on=['year', 'event', 'driver_code', 'driver_number'],\n",
        "        right_on=['year', 'name', 'code', 'number_driver'],\n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Sort by driver and raceId (raceId is in chronological order, more reliable than date)\n",
        "    # Use raceId for temporal ordering since it's guaranteed to be sequential\n",
        "    metrics_sorted = metrics_with_master.sort_values(['driver_code', 'driver_number', 'raceId']).reset_index(drop=True)\n",
        "    \n",
        "    # Define relative features to standardize\n",
        "    relative_features = [\n",
        "        'drs_activation_rate',\n",
        "        'position_change_rate',\n",
        "        'lap_time_std',\n",
        "        'first_pit_lap',\n",
        "        'sector_speed_laptime_corr',\n",
        "        'tyre_efficiency_index'\n",
        "    ]\n",
        "    \n",
        "    # Standardize relative features per race (z-score within each race)\n",
        "    logger.info(\"Standardizing relative features per race...\")\n",
        "    \n",
        "    standardized_metrics = []\n",
        "    \n",
        "    # Group by raceId (more reliable than year+event)\n",
        "    for race_id, race_group in metrics_sorted.groupby('raceId'):\n",
        "        if pd.isna(race_id):\n",
        "            continue\n",
        "            \n",
        "        race_metrics = race_group.copy()\n",
        "        \n",
        "        # Standardize each relative feature within this race\n",
        "        for feature in relative_features:\n",
        "            if feature not in race_metrics.columns:\n",
        "                continue\n",
        "            \n",
        "            feature_values = race_metrics[feature].values\n",
        "            feature_mean = np.nanmean(feature_values)\n",
        "            feature_std = np.nanstd(feature_values)\n",
        "            \n",
        "            if feature_std > 0:\n",
        "                # Z-score standardization\n",
        "                race_metrics[f'{feature}_relative'] = (feature_values - feature_mean) / feature_std\n",
        "            else:\n",
        "                race_metrics[f'{feature}_relative'] = 0.0\n",
        "        \n",
        "        standardized_metrics.append(race_metrics)\n",
        "    \n",
        "    result_df = pd.concat(standardized_metrics, ignore_index=True)\n",
        "    \n",
        "    # Calculate rolling averages per driver (ONLY previous races, shifted by 1)\n",
        "    # Use raceId for proper chronological ordering\n",
        "    result_df = result_df.sort_values(['driver_code', 'driver_number', 'raceId']).reset_index(drop=True)\n",
        "    \n",
        "    logger.info(\"Calculating rolling averages from previous races only (using raceId for ordering)...\")\n",
        "    \n",
        "    for feature in relative_features:\n",
        "        if feature not in result_df.columns:\n",
        "            continue\n",
        "        \n",
        "        for driver_code in result_df['driver_code'].unique():\n",
        "            driver_mask = result_df['driver_code'] == driver_code\n",
        "            driver_data = result_df[driver_mask].copy()\n",
        "            \n",
        "            # IMPORTANT: Use shift(1) to exclude current race from rolling average\n",
        "            # This ensures we only use PREVIOUS races for prediction\n",
        "            # raceId ordering ensures chronological correctness\n",
        "            rolling_3 = driver_data[feature].shift(1).rolling(window=3, min_periods=1).mean()\n",
        "            rolling_5 = driver_data[feature].shift(1).rolling(window=5, min_periods=1).mean()\n",
        "            rolling_10 = driver_data[feature].shift(1).rolling(window=10, min_periods=1).mean()\n",
        "            \n",
        "            result_df.loc[driver_mask, f'{feature}_avg_last_3'] = rolling_3.values\n",
        "            result_df.loc[driver_mask, f'{feature}_avg_last_5'] = rolling_5.values\n",
        "            result_df.loc[driver_mask, f'{feature}_avg_last_10'] = rolling_10.values\n",
        "    \n",
        "    logger.info(f\"Rolling averages and standardization complete. Shape: {result_df.shape}\")\n",
        "    \n",
        "    return result_df\n",
        "\n",
        "# Calculate rolling averages and standardize\n",
        "if 'historical_metrics' in globals() and len(historical_metrics) > 0:\n",
        "    metrics_with_rolling = calculate_rolling_averages_and_standardize(historical_metrics, master_2018plus)\n",
        "    logger.info(f\"Metrics with rolling averages: {metrics_with_rolling.shape}\")\n",
        "    logger.info(f\"New columns: {[c for c in metrics_with_rolling.columns if 'relative' in c or 'avg_last' in c][:10]}\")\n",
        "else:\n",
        "    logger.warning(\"No historical metrics to process!\")\n",
        "    metrics_with_rolling = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Calculate Weather Features\n",
        "\n",
        "Calculate weather features from WEATHER data (Session='R'): averages, rainfall flags, transitions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10:18:57 |     INFO | Calculating weather features...\n",
            "10:18:57 |     INFO | Combined weather data: 72,836 rows\n",
            "10:18:58 |     INFO | Weather features calculated for 149 races\n",
            "10:18:58 |     INFO | Weather features: (149, 11)\n",
            "10:18:58 |     INFO | Weather feature columns: ['year', 'event', 'raceId', 'weather_airtemp_avg', 'weather_tracktemp_avg', 'weather_humidity_avg', 'weather_pressure_avg', 'weather_windspeed_avg', 'weather_rainfall', 'weather_rainfall_transitions', 'weather_dry_wet_start']\n"
          ]
        }
      ],
      "source": [
        "def calculate_weather_features(weather_data: Dict, master_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculate weather features for each race.\n",
        "    \n",
        "    Features:\n",
        "    - AirTemp (average)\n",
        "    - TrackTemp (average)\n",
        "    - Humidity (average)\n",
        "    - Pressure (average)\n",
        "    - WindSpeed (average)\n",
        "    - Rainfall (bool - any rain during race)\n",
        "    - Rainfall transitions (count of drywet or wetdry)\n",
        "    - Dry vs wet start flag (rainfall at race start)\n",
        "    \"\"\"\n",
        "    logger.info(\"Calculating weather features...\")\n",
        "    \n",
        "    # Combine all weather data\n",
        "    all_weather = []\n",
        "    for year in sorted(weather_data.keys()):\n",
        "        if year in weather_data:\n",
        "            df = weather_data[year].copy()\n",
        "            all_weather.append(df)\n",
        "    \n",
        "    if not all_weather:\n",
        "        logger.warning(\"No weather data available!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    combined_weather = pd.concat(all_weather, ignore_index=True)\n",
        "    logger.info(f\"Combined weather data: {len(combined_weather):,} rows\")\n",
        "    \n",
        "    # Filter for race sessions only\n",
        "    race_weather = combined_weather[combined_weather['Session'] == 'R'].copy()\n",
        "    \n",
        "    if len(race_weather) == 0:\n",
        "        logger.warning(\"No race weather data available!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Calculate weather features per race\n",
        "    # Use raceId if available for more efficient grouping\n",
        "    weather_features = []\n",
        "    \n",
        "    if 'raceId' in race_weather.columns:\n",
        "        # Group by raceId (more reliable and efficient)\n",
        "        for race_id, race_weather_group in race_weather.groupby('raceId'):\n",
        "            if pd.isna(race_id):\n",
        "                continue\n",
        "            \n",
        "            # Get year and event from first row\n",
        "            first_row = race_weather_group.iloc[0]\n",
        "            year = int(first_row['Year']) if 'Year' in first_row.index else None\n",
        "            event = normalize_event_name(first_row['Event']) if 'Event' in first_row.index else None\n",
        "            \n",
        "            if year is None or event is None:\n",
        "                continue\n",
        "            \n",
        "            race_metrics = {\n",
        "                'year': year,\n",
        "                'event': event,\n",
        "                'raceId': int(race_id),\n",
        "            }\n",
        "            \n",
        "            # Average values over race\n",
        "            if 'AirTemp' in race_weather_group.columns:\n",
        "                race_metrics['weather_airtemp_avg'] = race_weather_group['AirTemp'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_airtemp_avg'] = np.nan\n",
        "            \n",
        "            if 'TrackTemp' in race_weather_group.columns:\n",
        "                race_metrics['weather_tracktemp_avg'] = race_weather_group['TrackTemp'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_tracktemp_avg'] = np.nan\n",
        "            \n",
        "            if 'Humidity' in race_weather_group.columns:\n",
        "                race_metrics['weather_humidity_avg'] = race_weather_group['Humidity'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_humidity_avg'] = np.nan\n",
        "            \n",
        "            if 'Pressure' in race_weather_group.columns:\n",
        "                race_metrics['weather_pressure_avg'] = race_weather_group['Pressure'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_pressure_avg'] = np.nan\n",
        "            \n",
        "            if 'WindSpeed' in race_weather_group.columns:\n",
        "                race_metrics['weather_windspeed_avg'] = race_weather_group['WindSpeed'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_windspeed_avg'] = np.nan\n",
        "            \n",
        "            # Rainfall flag (any rain during race)\n",
        "            if 'Rainfall' in race_weather_group.columns:\n",
        "                race_metrics['weather_rainfall'] = race_weather_group['Rainfall'].any() if race_weather_group['Rainfall'].dtype == bool else (race_weather_group['Rainfall'] == True).any()\n",
        "            else:\n",
        "                race_metrics['weather_rainfall'] = False\n",
        "            \n",
        "            # Rainfall transitions (drywet or wetdry)\n",
        "            if 'Rainfall' in race_weather_group.columns:\n",
        "                # Sort by time\n",
        "                race_weather_sorted = race_weather_group.sort_values('Time' if 'Time' in race_weather_group.columns else race_weather_group.index)\n",
        "                rainfall_values = race_weather_sorted['Rainfall']\n",
        "                \n",
        "                # Count transitions\n",
        "                transitions = 0\n",
        "                prev_value = None\n",
        "                for val in rainfall_values:\n",
        "                    if prev_value is not None and val != prev_value:\n",
        "                        transitions += 1\n",
        "                    prev_value = val\n",
        "                \n",
        "                race_metrics['weather_rainfall_transitions'] = transitions\n",
        "            else:\n",
        "                race_metrics['weather_rainfall_transitions'] = 0\n",
        "            \n",
        "            # Dry vs wet start flag (rainfall at race start)\n",
        "            if 'Rainfall' in race_weather_group.columns and 'Time' in race_weather_group.columns:\n",
        "                # Get first time entry\n",
        "                race_weather_sorted = race_weather_group.sort_values('Time')\n",
        "                first_row = race_weather_sorted.iloc[0]\n",
        "                race_metrics['weather_dry_wet_start'] = bool(first_row['Rainfall']) if pd.notna(first_row['Rainfall']) else False\n",
        "            elif 'Rainfall' in race_weather_group.columns:\n",
        "                # Use first entry\n",
        "                first_row = race_weather_group.iloc[0]\n",
        "                race_metrics['weather_dry_wet_start'] = bool(first_row['Rainfall']) if pd.notna(first_row['Rainfall']) else False\n",
        "            else:\n",
        "                race_metrics['weather_dry_wet_start'] = False\n",
        "            \n",
        "            weather_features.append(race_metrics)\n",
        "    else:\n",
        "        # Fallback to Year + Event grouping\n",
        "        for (year, event), race_weather_group in race_weather.groupby(['Year', 'Event']):\n",
        "            race_metrics = {\n",
        "                'year': year,\n",
        "                'event': normalize_event_name(event),\n",
        "            }\n",
        "            \n",
        "            # Average values over race\n",
        "            if 'AirTemp' in race_weather_group.columns:\n",
        "                race_metrics['weather_airtemp_avg'] = race_weather_group['AirTemp'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_airtemp_avg'] = np.nan\n",
        "            \n",
        "            if 'TrackTemp' in race_weather_group.columns:\n",
        "                race_metrics['weather_tracktemp_avg'] = race_weather_group['TrackTemp'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_tracktemp_avg'] = np.nan\n",
        "            \n",
        "            if 'Humidity' in race_weather_group.columns:\n",
        "                race_metrics['weather_humidity_avg'] = race_weather_group['Humidity'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_humidity_avg'] = np.nan\n",
        "            \n",
        "            if 'Pressure' in race_weather_group.columns:\n",
        "                race_metrics['weather_pressure_avg'] = race_weather_group['Pressure'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_pressure_avg'] = np.nan\n",
        "            \n",
        "            if 'WindSpeed' in race_weather_group.columns:\n",
        "                race_metrics['weather_windspeed_avg'] = race_weather_group['WindSpeed'].mean()\n",
        "            else:\n",
        "                race_metrics['weather_windspeed_avg'] = np.nan\n",
        "            \n",
        "            # Rainfall flag (any rain during race)\n",
        "            if 'Rainfall' in race_weather_group.columns:\n",
        "                race_metrics['weather_rainfall'] = race_weather_group['Rainfall'].any() if race_weather_group['Rainfall'].dtype == bool else (race_weather_group['Rainfall'] == True).any()\n",
        "            else:\n",
        "                race_metrics['weather_rainfall'] = False\n",
        "            \n",
        "            # Rainfall transitions (drywet or wetdry)\n",
        "            if 'Rainfall' in race_weather_group.columns:\n",
        "                # Sort by time\n",
        "                race_weather_sorted = race_weather_group.sort_values('Time' if 'Time' in race_weather_group.columns else race_weather_group.index)\n",
        "                rainfall_values = race_weather_sorted['Rainfall']\n",
        "                \n",
        "                # Count transitions\n",
        "                transitions = 0\n",
        "                prev_value = None\n",
        "                for val in rainfall_values:\n",
        "                    if prev_value is not None and val != prev_value:\n",
        "                        transitions += 1\n",
        "                    prev_value = val\n",
        "                \n",
        "                race_metrics['weather_rainfall_transitions'] = transitions\n",
        "            else:\n",
        "                race_metrics['weather_rainfall_transitions'] = 0\n",
        "            \n",
        "            # Dry vs wet start flag (rainfall at race start)\n",
        "            if 'Rainfall' in race_weather_group.columns and 'Time' in race_weather_group.columns:\n",
        "                # Get first time entry\n",
        "                race_weather_sorted = race_weather_group.sort_values('Time')\n",
        "                first_row = race_weather_sorted.iloc[0]\n",
        "                race_metrics['weather_dry_wet_start'] = bool(first_row['Rainfall']) if pd.notna(first_row['Rainfall']) else False\n",
        "            elif 'Rainfall' in race_weather_group.columns:\n",
        "                # Use first entry\n",
        "                first_row = race_weather_group.iloc[0]\n",
        "                race_metrics['weather_dry_wet_start'] = bool(first_row['Rainfall']) if pd.notna(first_row['Rainfall']) else False\n",
        "            else:\n",
        "                race_metrics['weather_dry_wet_start'] = False\n",
        "            \n",
        "            weather_features.append(race_metrics)\n",
        "    \n",
        "    weather_df = pd.DataFrame(weather_features)\n",
        "    \n",
        "    logger.info(f\"Weather features calculated for {len(weather_df):,} races\")\n",
        "    \n",
        "    return weather_df\n",
        "\n",
        "# Calculate weather features\n",
        "if len(fastf1_data['WEATHER']) > 0:\n",
        "    weather_features = calculate_weather_features(fastf1_data['WEATHER'], master_2018plus)\n",
        "    logger.info(f\"Weather features: {weather_features.shape}\")\n",
        "    logger.info(f\"Weather feature columns: {list(weather_features.columns)}\")\n",
        "else:\n",
        "    logger.warning(\"No weather data available!\")\n",
        "    weather_features = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Merge Features to Master CSV\n",
        "\n",
        "Merge all calculated features back to master_races_augmented.csv.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10:18:58 |     INFO | Merging features to master CSV...\n",
            "10:18:58 |     INFO | Merged 37 metric features\n",
            "10:18:58 |     INFO | Merged 8 weather features (using raceId)\n",
            "10:18:58 |     INFO | Merge complete. Master shape: (12358, 165)\n",
            "10:18:58 |     INFO | New features added: 45\n",
            "10:18:58 |     INFO |   race_date: 9,379 missing (75.9%)\n",
            "10:18:58 |     INFO |   race_id: 9,379 missing (75.9%)\n",
            "10:18:58 |     INFO |   driver_id: 9,379 missing (75.9%)\n",
            "10:18:58 |     INFO |   position_changes: 9,546 missing (77.2%)\n",
            "10:18:58 |     INFO |   position_change_rate: 9,546 missing (77.2%)\n",
            "10:18:58 |     INFO |   lap_time_mean: 9,530 missing (77.1%)\n",
            "10:18:58 |     INFO |   lap_time_std: 9,569 missing (77.4%)\n",
            "10:18:58 |     INFO |   first_pit_lap: 9,659 missing (78.2%)\n",
            "10:18:58 |     INFO |   pit_stops: 9,379 missing (75.9%)\n",
            "10:18:58 |     INFO |   sector_speed_laptime_corr: 9,573 missing (77.5%)\n",
            "10:18:58 |     INFO | Master with features: (12358, 165)\n",
            "10:18:58 |     INFO | New features added: 45\n",
            "10:18:58 |     INFO | New feature names: ['driver_id', 'drs_activation_rate', 'drs_activation_rate_avg_last_10', 'drs_activation_rate_avg_last_3', 'drs_activation_rate_avg_last_5', 'drs_activation_rate_relative', 'drs_time_fraction', 'first_pit_lap', 'first_pit_lap_avg_last_10', 'first_pit_lap_avg_last_3', 'first_pit_lap_avg_last_5', 'first_pit_lap_relative', 'lap_time_mean', 'lap_time_std', 'lap_time_std_avg_last_10', 'lap_time_std_avg_last_3', 'lap_time_std_avg_last_5', 'lap_time_std_relative', 'pit_stops', 'position_change_rate']...\n"
          ]
        }
      ],
      "source": [
        "def merge_features_to_master(master_df: pd.DataFrame, metrics_df: pd.DataFrame, \n",
        "                            weather_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Merge all FastF1 features to master CSV.\n",
        "    \n",
        "    Join on: year + name (normalized) + code + number_driver\n",
        "    \"\"\"\n",
        "    logger.info(\"Merging features to master CSV...\")\n",
        "    \n",
        "    master_result = master_df.copy()\n",
        "    \n",
        "    # Prepare metrics_df for merge (need to match on master CSV columns)\n",
        "    if len(metrics_df) > 0:\n",
        "        # Prepare DataFrames with consistent types for merging\n",
        "        metrics_df_prep = metrics_df.copy()\n",
        "        if 'driver_number' in metrics_df_prep.columns:\n",
        "            metrics_df_prep['driver_number'] = pd.to_numeric(metrics_df_prep['driver_number'], errors='coerce')\n",
        "        \n",
        "        master_df_prep = master_df[['year', 'name', 'code', 'number_driver', 'raceId', 'driverId', 'date']].copy()\n",
        "        master_df_prep['number_driver'] = pd.to_numeric(master_df_prep['number_driver'], errors='coerce')\n",
        "        \n",
        "        # Merge metrics on year + event + driver code + driver number\n",
        "        metrics_for_merge = metrics_df_prep.merge(\n",
        "            master_df_prep,\n",
        "            left_on=['year', 'event', 'driver_code', 'driver_number'],\n",
        "            right_on=['year', 'name', 'code', 'number_driver'],\n",
        "            how='right',\n",
        "            suffixes=('', '_master')\n",
        "        )\n",
        "        \n",
        "        # Select feature columns to merge (exclude join keys already in master)\n",
        "        feature_cols = [c for c in metrics_for_merge.columns \n",
        "                       if c not in ['year', 'event', 'driver_code', 'driver_number', 'name', 'code', 'number_driver'] \n",
        "                       and c not in master_df.columns\n",
        "                       and not c.endswith('_master')]\n",
        "        \n",
        "        if feature_cols:\n",
        "            # Merge on raceId and driverId for exact matching\n",
        "            metrics_merge = metrics_for_merge[['raceId', 'driverId'] + feature_cols].copy()\n",
        "            master_result = master_result.merge(\n",
        "                metrics_merge,\n",
        "                on=['raceId', 'driverId'],\n",
        "                how='left'\n",
        "            )\n",
        "            logger.info(f\"Merged {len(feature_cols)} metric features\")\n",
        "    else:\n",
        "        logger.warning(\"No metrics to merge!\")\n",
        "    \n",
        "    # Prepare weather_df for merge (one row per race, not per driver)\n",
        "    if len(weather_df) > 0:\n",
        "        # Use raceId if available for more efficient merging\n",
        "        if 'raceId' in weather_df.columns:\n",
        "            # Merge directly on raceId\n",
        "            weather_cols = [c for c in weather_df.columns \n",
        "                           if c not in ['year', 'event', 'raceId'] \n",
        "                           and c not in master_df.columns]\n",
        "            \n",
        "            if weather_cols:\n",
        "                weather_merge = weather_df[['raceId'] + weather_cols].copy()\n",
        "                master_result = master_result.merge(\n",
        "                    weather_merge,\n",
        "                    on='raceId',\n",
        "                    how='left'\n",
        "                )\n",
        "                logger.info(f\"Merged {len(weather_cols)} weather features (using raceId)\")\n",
        "        else:\n",
        "            # Fallback to year + name matching\n",
        "            weather_for_merge = weather_df.merge(\n",
        "                master_df[['year', 'name', 'raceId']].drop_duplicates(),\n",
        "                left_on=['year', 'event'],\n",
        "                right_on=['year', 'name'],\n",
        "                how='right'\n",
        "            )\n",
        "            \n",
        "            weather_cols = [c for c in weather_for_merge.columns \n",
        "                           if c not in ['year', 'event', 'name'] \n",
        "                           and c not in master_df.columns]\n",
        "            \n",
        "            if weather_cols:\n",
        "                weather_merge = weather_for_merge[['raceId'] + weather_cols].copy()\n",
        "                master_result = master_result.merge(\n",
        "                    weather_merge,\n",
        "                    on='raceId',\n",
        "                    how='left'\n",
        "                )\n",
        "                logger.info(f\"Merged {len(weather_cols)} weather features (using year+name)\")\n",
        "    else:\n",
        "        logger.warning(\"No weather features to merge!\")\n",
        "    \n",
        "    logger.info(f\"Merge complete. Master shape: {master_result.shape}\")\n",
        "    \n",
        "    # Count missing data\n",
        "    if len(metrics_df) > 0 or len(weather_df) > 0:\n",
        "        new_features = [c for c in master_result.columns if c not in master_df.columns]\n",
        "        logger.info(f\"New features added: {len(new_features)}\")\n",
        "        \n",
        "        for feature in new_features[:10]:  # Show first 10\n",
        "            missing = master_result[feature].isna().sum()\n",
        "            missing_pct = (missing / len(master_result)) * 100\n",
        "            logger.info(f\"  {feature}: {missing:,} missing ({missing_pct:.1f}%)\")\n",
        "    \n",
        "    return master_result\n",
        "\n",
        "# Merge all features\n",
        "if ('metrics_with_rolling' in globals() and len(metrics_with_rolling) > 0) or ('weather_features' in globals() and len(weather_features) > 0):\n",
        "    metrics_df_to_merge = metrics_with_rolling if 'metrics_with_rolling' in globals() else pd.DataFrame()\n",
        "    weather_df_to_merge = weather_features if 'weather_features' in globals() else pd.DataFrame()\n",
        "    \n",
        "    master_with_features = merge_features_to_master(master, metrics_df_to_merge, weather_df_to_merge)\n",
        "    logger.info(f\"Master with features: {master_with_features.shape}\")\n",
        "    \n",
        "    # Compare to original\n",
        "    original_cols = set(master.columns)\n",
        "    new_cols = set(master_with_features.columns) - original_cols\n",
        "    logger.info(f\"New features added: {len(new_cols)}\")\n",
        "    logger.info(f\"New feature names: {sorted(list(new_cols))[:20]}...\")  # Show first 20\n",
        "else:\n",
        "    logger.warning(\"No features to merge!\")\n",
        "    master_with_features = master.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Analyze Missing Data (2018+)\n",
        "\n",
        "Analyze missingness of FastF1 features for 2018+ data to understand data coverage and quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12:26:02 |     INFO | ============================================================\n",
            "12:26:02 |     INFO | MISSING DATA ANALYSIS (2018+)\n",
            "12:26:02 |     INFO | ============================================================\n",
            "12:26:02 |     INFO | \n",
            "Total rows (2018+): 2,979\n",
            "12:26:02 |     INFO | Date range: 2018-03-25 00:00:00 to 2024-12-08 00:00:00\n",
            "12:26:02 |     INFO | \n",
            "FastF1 features added: 45\n",
            "12:26:02 |     INFO | \n",
            "============================================================\n",
            "12:26:02 |     INFO | MISSING DATA SUMMARY (2018+)\n",
            "12:26:02 |     INFO | ============================================================\n",
            "12:26:02 |     INFO | \n",
            "Top 10 features with highest missingness:\n",
            "12:26:02 |     INFO |   drs_activation_rate                      | Missing: 2,713/2,979 ( 91.07%)\n",
            "12:26:02 |     INFO |   drs_time_fraction                        | Missing: 2,713/2,979 ( 91.07%)\n",
            "12:26:02 |     INFO |   drs_activation_rate_avg_last_3           | Missing: 2,673/2,979 ( 89.73%)\n",
            "12:26:02 |     INFO |   drs_activation_rate_avg_last_5           | Missing: 2,633/2,979 ( 88.39%)\n",
            "12:26:02 |     INFO |   drs_activation_rate_avg_last_10          | Missing: 2,539/2,979 ( 85.23%)\n",
            "12:26:02 |     INFO |   first_pit_lap                            | Missing:   280/2,979 (  9.40%)\n",
            "12:26:02 |     INFO |   first_pit_lap_relative                   | Missing:   241/2,979 (  8.09%)\n",
            "12:26:02 |     INFO |   sector_speed_laptime_corr                | Missing:   194/2,979 (  6.51%)\n",
            "12:26:02 |     INFO |   tyre_efficiency_index                    | Missing:   191/2,979 (  6.41%)\n",
            "12:26:02 |     INFO |   lap_time_std                             | Missing:   190/2,979 (  6.38%)\n",
            "12:26:02 |     INFO | \n",
            "Top 10 features with lowest missingness:\n",
            "12:26:02 |     INFO |   pit_stops                                | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   race_id                                  | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   weather_airtemp_avg                      | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   weather_dry_wet_start                    | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   weather_humidity_avg                     | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   weather_pressure_avg                     | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   weather_rainfall                         | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   weather_rainfall_transitions             | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   weather_tracktemp_avg                    | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO |   weather_windspeed_avg                    | Missing:     0/2,979 (  0.00%)\n",
            "12:26:02 |     INFO | \n",
            "============================================================\n",
            "12:26:02 |     INFO | MISSING DATA BY FEATURE TYPE\n",
            "12:26:02 |     INFO | ============================================================\n",
            "12:26:02 |     INFO | \n",
            "DRS Patterns (6 features):\n",
            "12:26:02 |     INFO |   Average missing: 74.33%\n",
            "12:26:02 |     INFO |   Features: drs_activation_rate, drs_time_fraction, drs_activation_rate_relative, drs_activation_rate_avg_last_3, drs_activation_rate_avg_last_5...\n",
            "12:26:02 |     INFO | \n",
            "Overtaking/Position (6 features):\n",
            "12:26:02 |     INFO |   Average missing: 4.62%\n",
            "12:26:02 |     INFO |   Features: position_changes, position_change_rate, position_change_rate_relative, position_change_rate_avg_last_3, position_change_rate_avg_last_5...\n",
            "12:26:02 |     INFO | \n",
            "Lap Time (6 features):\n",
            "12:26:02 |     INFO |   Average missing: 4.67%\n",
            "12:26:02 |     INFO |   Features: lap_time_mean, lap_time_std, lap_time_std_relative, lap_time_std_avg_last_3, lap_time_std_avg_last_5...\n",
            "12:26:02 |     INFO | \n",
            "Pit Stop (6 features):\n",
            "12:26:02 |     INFO |   Average missing: 4.83%\n",
            "12:26:02 |     INFO |   Features: first_pit_lap, pit_stops, first_pit_lap_relative, first_pit_lap_avg_last_3, first_pit_lap_avg_last_5...\n",
            "12:26:02 |     INFO | \n",
            "Sector Speed (5 features):\n",
            "12:26:02 |     INFO |   Average missing: 4.65%\n",
            "12:26:02 |     INFO |   Features: sector_speed_laptime_corr, sector_speed_laptime_corr_relative, sector_speed_laptime_corr_avg_last_3, sector_speed_laptime_corr_avg_last_5, sector_speed_laptime_corr_avg_last_10\n",
            "12:26:02 |     INFO | \n",
            "Tyre Efficiency (5 features):\n",
            "12:26:02 |     INFO |   Average missing: 4.61%\n",
            "12:26:02 |     INFO |   Features: tyre_efficiency_index, tyre_efficiency_index_relative, tyre_efficiency_index_avg_last_3, tyre_efficiency_index_avg_last_5, tyre_efficiency_index_avg_last_10\n",
            "12:26:02 |     INFO | \n",
            "Weather (8 features):\n",
            "12:26:02 |     INFO |   Average missing: 0.00%\n",
            "12:26:02 |     INFO |   Features: weather_airtemp_avg, weather_tracktemp_avg, weather_humidity_avg, weather_pressure_avg, weather_windspeed_avg...\n",
            "12:26:02 |     INFO | \n",
            "Relative Features (6 features):\n",
            "12:26:02 |     INFO |   Average missing: 5.24%\n",
            "12:26:02 |     INFO |   Features: drs_activation_rate_relative, position_change_rate_relative, lap_time_std_relative, first_pit_lap_relative, sector_speed_laptime_corr_relative...\n",
            "12:26:02 |     INFO | \n",
            "Rolling Averages (18 features):\n",
            "12:26:02 |     INFO |   Average missing: 17.68%\n",
            "12:26:02 |     INFO |   Features: drs_activation_rate_avg_last_3, drs_activation_rate_avg_last_5, drs_activation_rate_avg_last_10, position_change_rate_avg_last_3, position_change_rate_avg_last_5...\n",
            "12:26:02 |     INFO | \n",
            "============================================================\n",
            "12:26:02 |     INFO | MISSING DATA BY YEAR (2018+)\n",
            "12:26:02 |     INFO | ============================================================\n",
            "12:26:02 |     INFO |   2018: 420 rows | Avg missing: 14.29%\n",
            "12:26:02 |     INFO |   2019: 420 rows | Avg missing: 11.93%\n",
            "12:26:02 |     INFO |   2020: 340 rows | Avg missing: 12.50%\n",
            "12:26:02 |     INFO |   2021: 440 rows | Avg missing: 12.78%\n",
            "12:26:02 |     INFO |   2022: 440 rows | Avg missing: 6.97%\n",
            "12:26:02 |     INFO |   2023: 440 rows | Avg missing: 15.61%\n",
            "12:26:02 |     INFO |   2024: 479 rows | Avg missing: 15.40%\n",
            "12:26:02 |     INFO | \n",
            "Missing data analysis saved to: C:\\Users\\Erik Viljamaa\\Downloads\\projects\\f1-podium-predictor\\data\\processed\\fastf1_missing_data_analysis_2018plus.csv\n"
          ]
        }
      ],
      "source": [
        "# Analyze missing data for 2018+ only\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(\"MISSING DATA ANALYSIS (2018+)\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "if 'master_with_features' in globals() and len(master_with_features) > 0:\n",
        "    # Filter to 2018+ data\n",
        "    master_2018plus_analysis = master_with_features[master_with_features['year'] >= 2018].copy()\n",
        "    \n",
        "    logger.info(f\"\\nTotal rows (2018+): {len(master_2018plus_analysis):,}\")\n",
        "    logger.info(f\"Date range: {master_2018plus_analysis['date'].min()} to {master_2018plus_analysis['date'].max()}\")\n",
        "    \n",
        "    # Identify FastF1 features (new columns added)\n",
        "    original_cols = set(master.columns)\n",
        "    fastf1_features = [col for col in master_with_features.columns if col not in original_cols]\n",
        "    \n",
        "    if len(fastf1_features) > 0:\n",
        "        logger.info(f\"\\nFastF1 features added: {len(fastf1_features)}\")\n",
        "        \n",
        "        # Analyze missingness for each feature\n",
        "        missing_analysis = []\n",
        "        \n",
        "        for feature in sorted(fastf1_features):\n",
        "            missing_count = master_2018plus_analysis[feature].isna().sum()\n",
        "            total_count = len(master_2018plus_analysis)\n",
        "            missing_pct = (missing_count / total_count * 100) if total_count > 0 else 0\n",
        "            \n",
        "            missing_analysis.append({\n",
        "                'feature': feature,\n",
        "                'missing_count': missing_count,\n",
        "                'total_count': total_count,\n",
        "                'missing_pct': missing_pct,\n",
        "                'available_count': total_count - missing_count,\n",
        "                'available_pct': 100 - missing_pct\n",
        "            })\n",
        "        \n",
        "        missing_df = pd.DataFrame(missing_analysis)\n",
        "        \n",
        "        # Display summary\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"MISSING DATA SUMMARY (2018+)\")\n",
        "        logger.info(\"=\"*60)\n",
        "        \n",
        "        # Sort by missing percentage (highest first)\n",
        "        missing_df_sorted = missing_df.sort_values('missing_pct', ascending=False)\n",
        "        \n",
        "        logger.info(\"\\nTop 10 features with highest missingness:\")\n",
        "        for idx, row in missing_df_sorted.head(10).iterrows():\n",
        "            logger.info(f\"  {row['feature']:40s} | Missing: {row['missing_count']:5,}/{row['total_count']:5,} ({row['missing_pct']:6.2f}%)\")\n",
        "        \n",
        "        logger.info(\"\\nTop 10 features with lowest missingness:\")\n",
        "        for idx, row in missing_df_sorted.tail(10).iterrows():\n",
        "            logger.info(f\"  {row['feature']:40s} | Missing: {row['missing_count']:5,}/{row['total_count']:5,} ({row['missing_pct']:6.2f}%)\")\n",
        "        \n",
        "        # Group by feature type\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"MISSING DATA BY FEATURE TYPE\")\n",
        "        logger.info(\"=\"*60)\n",
        "        \n",
        "        feature_categories = {\n",
        "            'DRS Patterns': [f for f in fastf1_features if 'drs' in f.lower()],\n",
        "            'Overtaking/Position': [f for f in fastf1_features if any(x in f.lower() for x in ['position', 'overtak'])],\n",
        "            'Lap Time': [f for f in fastf1_features if 'lap_time' in f.lower()],\n",
        "            'Pit Stop': [f for f in fastf1_features if 'pit' in f.lower()],\n",
        "            'Sector Speed': [f for f in fastf1_features if 'sector' in f.lower()],\n",
        "            'Tyre Efficiency': [f for f in fastf1_features if 'tyre' in f.lower()],\n",
        "            'Weather': [f for f in fastf1_features if 'weather' in f.lower()],\n",
        "            'Relative Features': [f for f in fastf1_features if '_relative' in f],\n",
        "            'Rolling Averages': [f for f in fastf1_features if '_avg_last_' in f],\n",
        "        }\n",
        "        \n",
        "        for category, features in feature_categories.items():\n",
        "            if features:\n",
        "                cat_missing = missing_df[missing_df['feature'].isin(features)]\n",
        "                if len(cat_missing) > 0:\n",
        "                    avg_missing_pct = cat_missing['missing_pct'].mean()\n",
        "                    logger.info(f\"\\n{category} ({len(features)} features):\")\n",
        "                    logger.info(f\"  Average missing: {avg_missing_pct:.2f}%\")\n",
        "                    logger.info(f\"  Features: {', '.join(features[:5])}{'...' if len(features) > 5 else ''}\")\n",
        "        \n",
        "        # Year-by-year analysis\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"MISSING DATA BY YEAR (2018+)\")\n",
        "        logger.info(\"=\"*60)\n",
        "        \n",
        "        for year in sorted(master_2018plus_analysis['year'].unique()):\n",
        "            year_data = master_2018plus_analysis[master_2018plus_analysis['year'] == year]\n",
        "            year_features_missing = {}\n",
        "            \n",
        "            for feature in fastf1_features:\n",
        "                missing_pct = (year_data[feature].isna().sum() / len(year_data) * 100) if len(year_data) > 0 else 100\n",
        "                year_features_missing[feature] = missing_pct\n",
        "            \n",
        "            avg_missing = np.mean(list(year_features_missing.values()))\n",
        "            logger.info(f\"  {year}: {len(year_data):,} rows | Avg missing: {avg_missing:.2f}%\")\n",
        "        \n",
        "        # Save missing data analysis to CSV\n",
        "        missing_analysis_path = PROCESSED_ROOT / \"fastf1_missing_data_analysis_2018plus.csv\"\n",
        "        missing_df_sorted.to_csv(missing_analysis_path, index=False)\n",
        "        logger.info(f\"\\nMissing data analysis saved to: {missing_analysis_path}\")\n",
        "        \n",
        "    else:\n",
        "        logger.warning(\"No FastF1 features found to analyze!\")\n",
        "else:\n",
        "    logger.warning(\"master_with_features not available for missing data analysis!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Export Augmented Dataset\n",
        "\n",
        "Save the augmented dataset with FastF1 features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10:18:58 |     INFO | Saving augmented dataset to C:\\Users\\Erik Viljamaa\\Downloads\\projects\\f1-podium-predictor\\data\\processed\\master_races_augmented.csv...\n",
            "10:18:59 |     INFO | ============================================================\n",
            "10:18:59 |     INFO | FEATURE ENGINEERING COMPLETE!\n",
            "10:18:59 |     INFO | ============================================================\n",
            "10:18:59 |     INFO | Original shape: (12358, 120)\n",
            "10:18:59 |     INFO | New shape: (12358, 165)\n",
            "10:18:59 |     INFO | Features added: 45\n",
            "10:18:59 |     INFO | \n",
            "New features summary:\n",
            "10:18:59 |     INFO |   driver_id: mean=694.881, std=309.250, missing=9379\n",
            "10:18:59 |     INFO |   drs_activation_rate: mean=0.434, std=0.191, missing=12092\n",
            "10:18:59 |     INFO |   drs_activation_rate_avg_last_10: mean=0.425, std=0.088, missing=11918\n",
            "10:18:59 |     INFO |   drs_activation_rate_avg_last_3: mean=0.431, std=0.111, missing=12052\n",
            "10:18:59 |     INFO |   drs_activation_rate_avg_last_5: mean=0.427, std=0.099, missing=12012\n",
            "10:18:59 |     INFO |   drs_activation_rate_relative: mean=0.000, std=0.300, missing=9393\n",
            "10:18:59 |     INFO |   drs_time_fraction: mean=0.434, std=0.191, missing=12092\n",
            "10:18:59 |     INFO |   first_pit_lap: mean=19.527, std=11.561, missing=9659\n",
            "10:18:59 |     INFO |   first_pit_lap_avg_last_10: mean=19.570, std=4.663, missing=9489\n",
            "10:18:59 |     INFO |   first_pit_lap_avg_last_3: mean=19.520, std=7.346, missing=9496\n",
            "10:18:59 |     INFO |   first_pit_lap_avg_last_5: mean=19.549, std=5.797, missing=9494\n",
            "10:18:59 |     INFO |   first_pit_lap_relative: mean=-0.000, std=0.971, missing=9620\n",
            "10:18:59 |     INFO |   lap_time_mean: mean=94.606, std=16.655, missing=9530\n",
            "10:18:59 |     INFO |   lap_time_std: mean=9.514, std=20.110, missing=9569\n",
            "10:18:59 |     INFO |   lap_time_std_avg_last_10: mean=9.450, std=5.988, missing=9483\n",
            "10:18:59 |     INFO |   lap_time_std_avg_last_3: mean=9.433, std=11.359, missing=9490\n",
            "10:18:59 |     INFO |   lap_time_std_avg_last_5: mean=9.434, std=8.857, missing=9488\n",
            "10:18:59 |     INFO |   lap_time_std_relative: mean=-0.000, std=0.997, missing=9549\n",
            "10:18:59 |     INFO |   pit_stops: mean=1.647, std=1.048, missing=9379\n",
            "10:18:59 |     INFO |   position_change_rate: mean=0.183, std=0.099, missing=9546\n",
            "10:18:59 |     INFO | \n",
            "Dataset saved to: C:\\Users\\Erik Viljamaa\\Downloads\\projects\\f1-podium-predictor\\data\\processed\\master_races_augmented.csv\n"
          ]
        }
      ],
      "source": [
        "# Export augmented dataset\n",
        "output_path = PROCESSED_ROOT / \"master_races_combined.csv\"\n",
        "\n",
        "logger.info(f\"Saving augmented dataset to {output_path}...\")\n",
        "master_with_features.to_csv(output_path, index=False)\n",
        "\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(\"FEATURE ENGINEERING COMPLETE!\")\n",
        "logger.info(\"=\"*60)\n",
        "logger.info(f\"Original shape: {master.shape}\")\n",
        "logger.info(f\"New shape: {master_with_features.shape}\")\n",
        "logger.info(f\"Features added: {len(set(master_with_features.columns) - set(master.columns))}\")\n",
        "\n",
        "# Display summary statistics\n",
        "new_features = [c for c in master_with_features.columns if c not in master.columns]\n",
        "if new_features:\n",
        "    logger.info(\"\\nNew features summary:\")\n",
        "    for feature in sorted(new_features)[:20]:  # Show first 20\n",
        "        if master_with_features[feature].dtype in [np.float64, np.int64]:\n",
        "            mean_val = master_with_features[feature].mean()\n",
        "            std_val = master_with_features[feature].std()\n",
        "            missing = master_with_features[feature].isna().sum()\n",
        "            logger.info(f\"  {feature}: mean={mean_val:.3f}, std={std_val:.3f}, missing={missing}\")\n",
        "\n",
        "logger.info(f\"\\nDataset saved to: {output_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
