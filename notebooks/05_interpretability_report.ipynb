{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 - Interpretability & Reporting\n",
        "\n",
        "Explain model predictions and provide insights using SHAP values and other interpretability techniques.\n",
        "\n",
        "**Sections:**\n",
        "1. **SHAP Values Analysis**: Global and local feature importance\n",
        "2. **Feature Impact Analysis**: Which features drive podium predictions\n",
        "3. **Case Studies**: Analyze specific race predictions\n",
        "4. **Model Calibration**: Calibration curve analysis\n",
        "5. **Business Insights**: Key findings and recommendations\n",
        "\n",
        "**Input:** `models/best_podium_model.pkl`, `data/processed/features.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score, brier_score_loss\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# SHAP for interpretability\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "    print(\"Warning: SHAP not available\")\n",
        "\n",
        "import joblib\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up paths\n",
        "# Get project root (works whether running from notebooks/ or F1/ folder)\n",
        "PROJECT_ROOT = Path().resolve()\n",
        "if PROJECT_ROOT.name == 'notebooks':\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "PROCESSED_ROOT = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "MODELS_ROOT = PROJECT_ROOT / \"models\"\n",
        "\n",
        "# Load model\n",
        "model_path = MODELS_ROOT / \"best_podium_model.pkl\"\n",
        "if not model_path.exists():\n",
        "    # Try to find any model file\n",
        "    model_files = list(MODELS_ROOT.glob(\"*.pkl\"))\n",
        "    if model_files:\n",
        "        model_path = model_files[0]\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No model file found in models/ directory\")\n",
        "\n",
        "model = joblib.load(model_path)\n",
        "print(f\"Loaded model from: {model_path}\")\n",
        "\n",
        "# Load features\n",
        "features = pd.read_csv(PROCESSED_ROOT / \"features.csv\")\n",
        "features['date'] = pd.to_datetime(features['date'], errors='coerce')\n",
        "\n",
        "# Prepare data\n",
        "X = features.drop(columns=['podium', 'raceId', 'driverId', 'date', 'year'], errors='ignore')\n",
        "y = features['podium']\n",
        "\n",
        "# Handle categorical features (same as in modeling notebook)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str).fillna('Unknown'))\n",
        "\n",
        "# Fill missing values\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in [np.float64, np.int64]:\n",
        "        X[col] = X[col].fillna(X[col].median())\n",
        "    else:\n",
        "        X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "# Test set (2024)\n",
        "test_mask = features['year'] == 2024\n",
        "X_test = X[test_mask]\n",
        "y_test = y[test_mask]\n",
        "features_test = features[test_mask].reset_index(drop=True)\n",
        "\n",
        "print(f\"Test set: {len(X_test):,} samples (2024)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Model Performance Summary\n",
        "\n",
        "Evaluate model performance on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictions on test set\n",
        "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
        "y_test_pred_binary = (y_test_pred >= 0.5).astype(int)\n",
        "\n",
        "# Metrics\n",
        "test_metrics = {\n",
        "    'roc_auc': roc_auc_score(y_test, y_test_pred),\n",
        "    'brier': brier_score_loss(y_test, y_test_pred)\n",
        "}\n",
        "\n",
        "print(\"Test Set Performance (2024):\")\n",
        "print(f\"  ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
        "print(f\"  Brier Score: {test_metrics['brier']:.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_test_pred_binary, target_names=['Non-Podium', 'Podium']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. SHAP Values Analysis\n",
        "\n",
        "Calculate SHAP values for global and local feature importance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE:\n",
        "    # Create SHAP explainer\n",
        "    # Use a sample of test data for faster computation\n",
        "    sample_size = min(100, len(X_test))\n",
        "    X_test_sample = X_test.iloc[:sample_size]\n",
        "    \n",
        "    try:\n",
        "        # Try TreeExplainer first (for LightGBM, CatBoost, XGBoost)\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(X_test_sample)\n",
        "        \n",
        "        # Handle binary classification (shap_values might be a list)\n",
        "        if isinstance(shap_values, list):\n",
        "            shap_values = shap_values[1]  # Use positive class\n",
        "        \n",
        "        print(f\"SHAP values calculated for {sample_size} samples\")\n",
        "        \n",
        "        # Global feature importance\n",
        "        shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\", show=False)\n",
        "        plt.title('Global Feature Importance (SHAP)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Detailed summary plot\n",
        "        shap.summary_plot(shap_values, X_test_sample, show=False)\n",
        "        plt.title('SHAP Summary Plot')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Feature importance DataFrame\n",
        "        feature_importance_shap = pd.DataFrame({\n",
        "            'feature': X_test_sample.columns,\n",
        "            'importance': np.abs(shap_values).mean(axis=0)\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(\"\\nTop 15 Features by SHAP Importance:\")\n",
        "        print(feature_importance_shap.head(15).to_string(index=False))\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"TreeExplainer failed: {e}\")\n",
        "        print(\"Trying KernelExplainer (slower but more general)...\")\n",
        "        try:\n",
        "            # Fallback to KernelExplainer\n",
        "            explainer = shap.KernelExplainer(model.predict_proba, X_test_sample.iloc[:50])\n",
        "            shap_values = explainer.shap_values(X_test_sample.iloc[:10])\n",
        "            \n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[1]\n",
        "            \n",
        "            shap.summary_plot(shap_values, X_test_sample.iloc[:10], plot_type=\"bar\", show=False)\n",
        "            plt.title('Global Feature Importance (SHAP - Kernel)')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "        except Exception as e2:\n",
        "            print(f\"KernelExplainer also failed: {e2}\")\n",
        "            print(\"SHAP analysis skipped\")\n",
        "            shap_values = None\n",
        "else:\n",
        "    print(\"SHAP not available - skipping SHAP analysis\")\n",
        "    shap_values = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Feature Impact Analysis\n",
        "\n",
        "Analyze which features drive podium predictions most.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Permutation importance as alternative to SHAP\n",
        "perm_result = permutation_importance(\n",
        "    model,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    n_repeats=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "perm_importances = pd.Series(perm_result.importances_mean, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "perm_importances.head(15).sort_values().plot.barh(color='steelblue')\n",
        "plt.title('Top 15 Feature Importances (Permutation)')\n",
        "plt.xlabel('Importance (Decrease in ROC-AUC)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 15 Features by Permutation Importance:\")\n",
        "print(perm_importances.head(15).to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Partial Dependence Plots\n",
        "\n",
        "Visualize how individual features affect predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Partial dependence plots for top features\n",
        "top_features = perm_importances.head(3).index.tolist()\n",
        "available_features = [f for f in top_features if f in X_test.columns]\n",
        "\n",
        "if available_features:\n",
        "    fig, axes = plt.subplots(1, len(available_features), figsize=(5 * len(available_features), 4))\n",
        "    if len(available_features) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for feature_name, axis in zip(available_features, axes):\n",
        "        try:\n",
        "            PartialDependenceDisplay.from_estimator(\n",
        "                model,\n",
        "                X_test,\n",
        "                features=[feature_name],\n",
        "                kind='average',\n",
        "                ax=axis\n",
        "            )\n",
        "            axis.set_title(f'Partial Dependence: {feature_name}')\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create PDP for {feature_name}: {e}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No suitable features found for partial dependence plots\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Case Studies\n",
        "\n",
        "Analyze specific race predictions - high confidence and surprising predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find high-confidence predictions\n",
        "high_confidence = features_test.copy()\n",
        "high_confidence['predicted_prob'] = y_test_pred\n",
        "high_confidence['actual'] = y_test\n",
        "high_confidence['error'] = abs(high_confidence['predicted_prob'] - high_confidence['actual'])\n",
        "\n",
        "# High confidence correct predictions\n",
        "high_conf_correct = high_confidence[\n",
        "    (high_confidence['predicted_prob'] > 0.8) & (high_confidence['actual'] == 1)\n",
        "].sort_values('predicted_prob', ascending=False)\n",
        "\n",
        "# High confidence incorrect (surprising)\n",
        "high_conf_incorrect = high_confidence[\n",
        "    (high_confidence['predicted_prob'] > 0.7) & (high_confidence['actual'] == 0)\n",
        "].sort_values('predicted_prob', ascending=False)\n",
        "\n",
        "print(\"High Confidence Correct Predictions (Top 5):\")\n",
        "if len(high_conf_correct) > 0:\n",
        "    display_cols = ['surname', 'circuit_name', 'grid', 'predicted_prob', 'actual']\n",
        "    available_cols = [c for c in display_cols if c in high_conf_correct.columns]\n",
        "    print(high_conf_correct[available_cols].head(5).to_string(index=False))\n",
        "else:\n",
        "    print(\"None found\")\n",
        "\n",
        "print(\"\\nSurprising Predictions (High Confidence but Wrong - Top 5):\")\n",
        "if len(high_conf_incorrect) > 0:\n",
        "    print(high_conf_incorrect[available_cols].head(5).to_string(index=False))\n",
        "else:\n",
        "    print(\"None found\")\n",
        "\n",
        "# Local SHAP explanations for a few examples\n",
        "if SHAP_AVAILABLE and shap_values is not None:\n",
        "    print(\"\\nLocal SHAP Explanations (Sample):\")\n",
        "    # Show SHAP waterfall for first high-confidence prediction\n",
        "    if len(high_conf_correct) > 0:\n",
        "        idx = high_conf_correct.index[0]\n",
        "        if idx < len(X_test_sample):\n",
        "            shap.waterfall_plot(\n",
        "                shap.Explanation(\n",
        "                    values=shap_values[idx],\n",
        "                    base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n",
        "                    data=X_test_sample.iloc[idx],\n",
        "                    feature_names=X_test_sample.columns\n",
        "                ),\n",
        "                show=False\n",
        "            )\n",
        "            plt.title(f'SHAP Explanation for High-Confidence Prediction')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Calibration\n",
        "\n",
        "Analyze model calibration - how well do predicted probabilities match observed frequencies?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calibration curve\n",
        "prob_true, prob_pred = calibration_curve(y_test, y_test_pred, n_bins=10)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Model')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
        "plt.xlabel('Mean Predicted Probability')\n",
        "plt.ylabel('Observed Frequency')\n",
        "plt.title('Model Calibration Curve (Test Set 2024)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Brier Score: {brier_score_loss(y_test, y_test_pred):.4f}\")\n",
        "print(\"(Lower is better - measures calibration)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Business Insights Summary\n",
        "\n",
        "Key findings and recommendations for feature engineering improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Key Insights:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1. Top Predictive Features:\")\n",
        "if 'feature_importance_shap' in locals():\n",
        "    print(\"   (Based on SHAP values)\")\n",
        "    print(feature_importance_shap.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"   (Based on Permutation Importance)\")\n",
        "    print(perm_importances.head(10).to_string())\n",
        "\n",
        "print(\"\\n2. Model Performance:\")\n",
        "print(f\"   Test ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
        "print(f\"   Test Brier Score: {test_metrics['brier']:.4f}\")\n",
        "\n",
        "print(\"\\n3. Recommendations:\")\n",
        "print(\"   - Focus on grid position and qualifying performance (strongest predictors)\")\n",
        "print(\"   - Driver and constructor historical performance are important\")\n",
        "print(\"   - Consider adding more FastF1 telemetry features (when available)\")\n",
        "print(\"   - Weather features may improve predictions for 2018+ races\")\n",
        "\n",
        "# Save insights\n",
        "insights = {\n",
        "    'top_features': perm_importances.head(20).to_dict(),\n",
        "    'test_metrics': test_metrics,\n",
        "    'calibration': {\n",
        "        'brier_score': float(brier_score_loss(y_test, y_test_pred))\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "insights_path = PROCESSED_ROOT / \"interpretability_insights.json\"\n",
        "with open(insights_path, 'w') as f:\n",
        "    json.dump(insights, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\nInsights saved to: {insights_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
