{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Data Combining\n",
        "\n",
        "This notebook combines all Kaggle CSVs into a single master table `master_races.csv` with one row per (raceId, driverId) for all races from 1994 onwards.\n",
        "\n",
        "**Key Features:**\n",
        "- Combines results, races, circuits, drivers, constructors, qualifying, standings, and sprint results\n",
        "- Intelligent column deduplication (removes duplicates if data is identical, prefixes if different)\n",
        "- Adds placeholder columns for future FastF1 features\n",
        "- Includes FastF1 data preview (for exploration, not combination)\n",
        "\n",
        "**Output:** `data/processed/master_races.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle data: C:\\Users\\erikv\\Downloads\\F1\\data\\raw\\kaggle\n",
            "FastF1 data: C:\\Users\\erikv\\Downloads\\F1\\data\\raw\\fastf1_2018plus\n",
            "Output: C:\\Users\\erikv\\Downloads\\F1\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up paths\n",
        "# Get project root (works whether running from notebooks/ or F1/ folder)\n",
        "PROJECT_ROOT = Path().resolve()\n",
        "if PROJECT_ROOT.name == 'notebooks':\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "KAGGLE_ROOT = PROJECT_ROOT / \"data\" / \"raw\" / \"kaggle\"\n",
        "FASTF1_ROOT = PROJECT_ROOT / \"data\" / \"raw\" / \"fastf1_2018plus\"\n",
        "PROCESSED_ROOT = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "PROCESSED_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Kaggle data: {KAGGLE_ROOT}\")\n",
        "print(f\"FastF1 data: {FASTF1_ROOT}\")\n",
        "print(f\"Output: {PROCESSED_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading & FastF1 Preview\n",
        "\n",
        "Load all Kaggle CSVs and preview FastF1 data structure (for exploration only).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded races: 1,125 rows, 18 columns\n",
            "Loaded results: 26,759 rows, 18 columns\n",
            "Loaded drivers: 861 rows, 9 columns\n",
            "Loaded constructors: 212 rows, 5 columns\n",
            "Loaded circuits: 77 rows, 9 columns\n",
            "Loaded qualifying: 10,494 rows, 9 columns\n",
            "Loaded driver_standings: 34,863 rows, 7 columns\n",
            "Loaded constructor_standings: 13,391 rows, 7 columns\n",
            "Loaded constructor_results: 12,625 rows, 5 columns\n",
            "Loaded sprint_results: 360 rows, 16 columns\n",
            "\n",
            "Total Kaggle datasets loaded: 10\n"
          ]
        }
      ],
      "source": [
        "# Modular function to load Kaggle CSV\n",
        "def load_kaggle_csv(name, path=None):\n",
        "    \"\"\"Load a Kaggle CSV file and return DataFrame with source tracking.\"\"\"\n",
        "    if path is None:\n",
        "        path = KAGGLE_ROOT / f\"{name}.csv\"\n",
        "    if not path.exists():\n",
        "        print(f\"Warning: {name}.csv not found at {path}\")\n",
        "        return None\n",
        "    df = pd.read_csv(path)\n",
        "    df.attrs['source'] = 'kaggle'\n",
        "    df.attrs['name'] = name\n",
        "    return df\n",
        "\n",
        "# Load all Kaggle CSVs\n",
        "kaggle_files = {\n",
        "    'races': 'races',\n",
        "    'results': 'results',\n",
        "    'drivers': 'drivers',\n",
        "    'constructors': 'constructors',\n",
        "    'circuits': 'circuits',\n",
        "    'qualifying': 'qualifying',\n",
        "    'driver_standings': 'driver_standings',\n",
        "    'constructor_standings': 'constructor_standings',\n",
        "    'constructor_results': 'constructor_results',\n",
        "    'sprint_results': 'sprint_results'\n",
        "}\n",
        "\n",
        "kaggle_data = {}\n",
        "for key, filename in kaggle_files.items():\n",
        "    df = load_kaggle_csv(filename)\n",
        "    if df is not None:\n",
        "        kaggle_data[key] = df\n",
        "        print(f\"Loaded {key}: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
        "\n",
        "print(f\"\\nTotal Kaggle datasets loaded: {len(kaggle_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview FastF1 data structure (for exploration, not combination)\n",
        "print(\"FastF1 Data Preview (2018-2024):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fastf1_years = range(2018, 2025)\n",
        "fastf1_datasets = ['RESULTS', 'LAPS', 'TELEMETRY', 'WEATHER']\n",
        "\n",
        "for dataset in fastf1_datasets:\n",
        "    print(f\"\\n{dataset} Dataset:\")\n",
        "    sample_loaded = False\n",
        "    for year in fastf1_years:\n",
        "        path = FASTF1_ROOT / f\"ALL_{dataset}_{year}.csv\"\n",
        "        if path.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(path, nrows=5, low_memory=False)  # Just preview\n",
        "                if not sample_loaded:\n",
        "                    print(f\"  Columns ({len(df.columns)}): {list(df.columns)[:10]}...\")\n",
        "                    print(f\"  Sample row:\")\n",
        "                    print(f\"    {df.iloc[0].to_dict()}\")\n",
        "                    sample_loaded = True\n",
        "                print(f\"  {year}: File exists\")\n",
        "            except Exception as e:\n",
        "                print(f\"  {year}: Error loading - {e}\")\n",
        "        else:\n",
        "            print(f\"  {year}: File not found\")\n",
        "    \n",
        "    if not sample_loaded:\n",
        "        print(f\"  No data files found for {dataset}\")\n",
        "\n",
        "print(\"\\nNote: FastF1 data will be explored separately for feature extraction.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Intelligent Column Deduplication Function\n",
        "\n",
        "This function handles duplicate column names AFTER merging:\n",
        "- **Merge first** with temporary suffixes to preserve all rows\n",
        "- **Then compare** columns to detect duplicates\n",
        "- If data is identical: drop duplicate column\n",
        "- If data differs: prefix with source CSV name\n",
        "\n",
        "**Why merge first?** This ensures we compare all rows (including those that don't match in both datasets) rather than only comparing rows that match in a pre-merge inner join.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column deduplication function created.\n",
            "This function handles duplicates AFTER merging to ensure all rows are considered.\n"
          ]
        }
      ],
      "source": [
        "def handle_duplicate_columns_after_merge(merged_df, source_name, temp_suffix='_temp'):\n",
        "    \"\"\"\n",
        "    Handle duplicate columns after merging by comparing suffixed columns.\n",
        "    \n",
        "    This function works on already-merged data, ensuring all rows are considered.\n",
        "    When pandas merges with suffixes, conflicting columns get suffixed.\n",
        "    \n",
        "    Parameters:\n",
        "    - merged_df: DataFrame after merge (with suffixed columns)\n",
        "    - source_name: Name of source CSV (for prefixing different columns)\n",
        "    - temp_suffix: Suffix used during merge (default '_temp')\n",
        "    \n",
        "    Returns:\n",
        "    - cleaned_df: DataFrame with duplicates handled (identical dropped, different prefixed)\n",
        "    - dropped_cols: List of columns that were dropped (identical data)\n",
        "    - renamed_cols: Dict of columns that were renamed (different data)\n",
        "    \"\"\"\n",
        "    if merged_df is None or merged_df.empty:\n",
        "        return merged_df, [], {}\n",
        "    \n",
        "    # Find columns that have the temp suffix (these are conflicts from the merge)\n",
        "    temp_cols = [col for col in merged_df.columns if col.endswith(temp_suffix)]\n",
        "    original_cols = [col[:-len(temp_suffix)] for col in temp_cols]\n",
        "    \n",
        "    columns_to_drop = []\n",
        "    rename_dict = {}\n",
        "    \n",
        "    for orig_col, temp_col in zip(original_cols, temp_cols):\n",
        "        if orig_col in merged_df.columns:\n",
        "            # Compare original and temp columns across all rows\n",
        "            # Handle NaN comparisons properly\n",
        "            both_exist_mask = merged_df[orig_col].notna() & merged_df[temp_col].notna()\n",
        "            orig_only_mask = merged_df[orig_col].notna() & merged_df[temp_col].isna()\n",
        "            temp_only_mask = merged_df[orig_col].isna() & merged_df[temp_col].notna()\n",
        "            \n",
        "            # Check if values are identical where both exist\n",
        "            if both_exist_mask.any():\n",
        "                identical = (merged_df.loc[both_exist_mask, orig_col] == merged_df.loc[both_exist_mask, temp_col]).all()\n",
        "            else:\n",
        "                identical = True  # No overlapping values to compare\n",
        "            \n",
        "            # If identical where both exist, and no mismatched NaN patterns, drop temp\n",
        "            if identical and not orig_only_mask.any() and not temp_only_mask.any():\n",
        "                # Data is identical across all rows, drop the temp column\n",
        "                columns_to_drop.append(temp_col)\n",
        "            else:\n",
        "                # Data differs or has different NaN patterns, rename temp column\n",
        "                rename_dict[temp_col] = f\"{source_name}_{orig_col}\"\n",
        "        else:\n",
        "            # Original column doesn't exist (shouldn't happen, but handle it)\n",
        "            # Just rename the temp column\n",
        "            rename_dict[temp_col] = f\"{source_name}_{orig_col}\"\n",
        "    \n",
        "    # Apply renaming and dropping\n",
        "    cleaned_df = merged_df.drop(columns=columns_to_drop)\n",
        "    cleaned_df = cleaned_df.rename(columns=rename_dict)\n",
        "    \n",
        "    return cleaned_df, columns_to_drop, rename_dict\n",
        "\n",
        "print(\"Column deduplication function created.\")\n",
        "print(\"This function handles duplicates AFTER merging to ensure all rows are considered.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base table (results): 26,759 rows, 18 columns\n",
            "After filtering to 1994+: 12,358 rows\n",
            "After merging circuits: 12,358 rows, 31 columns\n",
            "After merging drivers: 12,358 rows, 39 columns\n",
            "After merging constructors: 12,358 rows, 43 columns\n",
            "\n",
            "Base table created: (12358, 43)\n",
            "Columns: ['resultId', 'raceId', 'driverId', 'constructorId', 'number', 'grid', 'position', 'positionText', 'positionOrder', 'points']... (43 total)\n"
          ]
        }
      ],
      "source": [
        "# Start with results.csv as base\n",
        "if 'results' not in kaggle_data:\n",
        "    raise ValueError(\"results.csv not found!\")\n",
        "\n",
        "master = kaggle_data['results'].copy()\n",
        "print(f\"Base table (results): {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "\n",
        "# Filter to year >= 1994 by merging with races first\n",
        "if 'races' in kaggle_data:\n",
        "    races = kaggle_data['races'].copy()\n",
        "    races['date'] = pd.to_datetime(races['date'], errors='coerce')\n",
        "    \n",
        "    # Merge races to get year\n",
        "    master = master.merge(\n",
        "        races[['raceId', 'year', 'round', 'circuitId', 'date', 'name']],\n",
        "        on='raceId',\n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Filter to 1994+\n",
        "    master = master[master['year'] >= 1994].copy()\n",
        "    print(f\"After filtering to 1994+: {master.shape[0]:,} rows\")\n",
        "    \n",
        "    # Merge circuits\n",
        "    if 'circuits' in kaggle_data:\n",
        "        circuits = kaggle_data['circuits'].copy()\n",
        "        # Rename 'name' column from circuits to avoid conflict with races 'name'\n",
        "        circuits = circuits.rename(columns={'name': 'circuit_name'})\n",
        "        master = master.merge(\n",
        "            circuits,\n",
        "            on='circuitId',\n",
        "            how='left',\n",
        "            suffixes=('', '_circuit')\n",
        "        )\n",
        "        print(f\"After merging circuits: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "    \n",
        "    # Merge drivers\n",
        "    if 'drivers' in kaggle_data:\n",
        "        drivers = kaggle_data['drivers'].copy()\n",
        "        master = master.merge(\n",
        "            drivers,\n",
        "            on='driverId',\n",
        "            how='left',\n",
        "            suffixes=('', '_driver')\n",
        "        )\n",
        "        print(f\"After merging drivers: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "    \n",
        "    # Merge constructors\n",
        "    if 'constructors' in kaggle_data:\n",
        "        constructors = kaggle_data['constructors'].copy()\n",
        "        master = master.merge(\n",
        "            constructors,\n",
        "            on='constructorId',\n",
        "            how='left',\n",
        "            suffixes=('', '_constructor')\n",
        "        )\n",
        "        print(f\"After merging constructors: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "\n",
        "print(f\"\\nBase table created: {master.shape}\")\n",
        "print(f\"Columns: {list(master.columns)[:10]}... ({len(master.columns)} total)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After merging driver_standings: 12,358 rows, 48 columns\n",
            "  Renamed different columns: ['driver_standings_points', 'driver_standings_position', 'driver_standings_positionText']...\n",
            "After merging constructor_standings: 12,358 rows, 53 columns\n",
            "  Renamed different columns: ['constructor_standings_points', 'constructor_standings_position', 'constructor_standings_positionText']...\n",
            "After merging constructor_results: 12,358 rows, 56 columns\n",
            "  Renamed different columns: ['constructor_results_points']...\n",
            "After merging qualifying: 12,358 rows, 63 columns\n",
            "  Renamed different columns: ['qualifying_constructorId', 'qualifying_number', 'qualifying_position']...\n",
            "After merging sprint_results: 12,358 rows, 77 columns\n",
            "  Renamed columns: ['sprint_results_resultId', 'sprint_results_constructorId', 'sprint_results_number', 'sprint_results_grid', 'sprint_results_position']...\n",
            "\n",
            "Master table after all merges: (12358, 77)\n"
          ]
        }
      ],
      "source": [
        "# Merge driver_standings\n",
        "if 'driver_standings' in kaggle_data:\n",
        "    driver_standings = kaggle_data['driver_standings'].copy()\n",
        "    \n",
        "    # Merge first with temp suffix\n",
        "    master = master.merge(\n",
        "        driver_standings,\n",
        "        on=['raceId', 'driverId'],\n",
        "        how='left',\n",
        "        suffixes=('', '_temp')\n",
        "    )\n",
        "    \n",
        "    # Handle duplicates after merge\n",
        "    master, dropped_cols, renamed_cols = handle_duplicate_columns_after_merge(\n",
        "        master, 'driver_standings', temp_suffix='_temp'\n",
        "    )\n",
        "    \n",
        "    print(f\"After merging driver_standings: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "    if dropped_cols:\n",
        "        print(f\"  Dropped identical columns: {[col.replace('_temp', '') for col in dropped_cols]}\")\n",
        "    if renamed_cols:\n",
        "        print(f\"  Renamed different columns: {list(renamed_cols.values())[:3]}...\")\n",
        "\n",
        "# Merge constructor_standings\n",
        "if 'constructor_standings' in kaggle_data:\n",
        "    constructor_standings = kaggle_data['constructor_standings'].copy()\n",
        "    \n",
        "    # Merge first with temp suffix\n",
        "    master = master.merge(\n",
        "        constructor_standings,\n",
        "        on=['raceId', 'constructorId'],\n",
        "        how='left',\n",
        "        suffixes=('', '_temp')\n",
        "    )\n",
        "    \n",
        "    # Handle duplicates after merge\n",
        "    master, dropped_cols, renamed_cols = handle_duplicate_columns_after_merge(\n",
        "        master, 'constructor_standings', temp_suffix='_temp'\n",
        "    )\n",
        "    \n",
        "    print(f\"After merging constructor_standings: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "    if dropped_cols:\n",
        "        print(f\"  Dropped identical columns: {[col.replace('_temp', '') for col in dropped_cols]}\")\n",
        "    if renamed_cols:\n",
        "        print(f\"  Renamed different columns: {list(renamed_cols.values())[:3]}...\")\n",
        "\n",
        "# Merge constructor_results\n",
        "if 'constructor_results' in kaggle_data:\n",
        "    constructor_results = kaggle_data['constructor_results'].copy()\n",
        "    \n",
        "    # Merge first with temp suffix\n",
        "    master = master.merge(\n",
        "        constructor_results,\n",
        "        on=['raceId', 'constructorId'],\n",
        "        how='left',\n",
        "        suffixes=('', '_temp')\n",
        "    )\n",
        "    \n",
        "    # Handle duplicates after merge\n",
        "    master, dropped_cols, renamed_cols = handle_duplicate_columns_after_merge(\n",
        "        master, 'constructor_results', temp_suffix='_temp'\n",
        "    )\n",
        "    \n",
        "    print(f\"After merging constructor_results: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "    if dropped_cols:\n",
        "        print(f\"  Dropped identical columns: {[col.replace('_temp', '') for col in dropped_cols]}\")\n",
        "    if renamed_cols:\n",
        "        print(f\"  Renamed different columns: {list(renamed_cols.values())[:3]}...\")\n",
        "\n",
        "# Merge qualifying\n",
        "if 'qualifying' in kaggle_data:\n",
        "    qualifying = kaggle_data['qualifying'].copy()\n",
        "    \n",
        "    # Merge first with temp suffix\n",
        "    master = master.merge(\n",
        "        qualifying,\n",
        "        on=['raceId', 'driverId'],\n",
        "        how='left',\n",
        "        suffixes=('', '_temp')\n",
        "    )\n",
        "    \n",
        "    # Handle duplicates after merge\n",
        "    master, dropped_cols, renamed_cols = handle_duplicate_columns_after_merge(\n",
        "        master, 'qualifying', temp_suffix='_temp'\n",
        "    )\n",
        "    \n",
        "    print(f\"After merging qualifying: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "    if dropped_cols:\n",
        "        print(f\"  Dropped identical columns: {[col.replace('_temp', '') for col in dropped_cols]}\")\n",
        "    if renamed_cols:\n",
        "        print(f\"  Renamed different columns: {list(renamed_cols.values())[:3]}...\")\n",
        "\n",
        "# Merge sprint_results (prefix all columns)\n",
        "if 'sprint_results' in kaggle_data:\n",
        "    sprint_results = kaggle_data['sprint_results'].copy()\n",
        "    \n",
        "    # For sprint_results, we want to prefix most columns except merge keys\n",
        "    merge_keys = ['raceId', 'driverId']\n",
        "    sprint_rename_dict = {}\n",
        "    for col in sprint_results.columns:\n",
        "        if col not in merge_keys:\n",
        "            sprint_rename_dict[col] = f\"sprint_results_{col}\"\n",
        "    \n",
        "    sprint_results_renamed = sprint_results.rename(columns=sprint_rename_dict)\n",
        "    \n",
        "    master = master.merge(\n",
        "        sprint_results_renamed,\n",
        "        on=['raceId', 'driverId'],\n",
        "        how='left',\n",
        "        suffixes=('', '_sprint')\n",
        "    )\n",
        "    print(f\"After merging sprint_results: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n",
        "    print(f\"  Renamed columns: {list(sprint_rename_dict.values())[:5]}...\")\n",
        "\n",
        "print(f\"\\nMaster table after all merges: {master.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output:**\n",
        "- Row counts after each merge (should remain constant - one row per raceId+driverId)\n",
        "- Column counts increasing as we add data from each source\n",
        "- Messages about dropped/renamed columns\n",
        "- **What to look for:**\n",
        "  - **Row count:** Should stay the same (~20K-25K rows) - merges are left joins on existing keys\n",
        "  - **Dropped columns:** These had identical data to existing columns (e.g., `points` might be identical in results and driver_standings)\n",
        "  - **Renamed columns:** These had different data, so we prefix them (e.g., `driver_standings_points` vs `results_points`)\n",
        "  - **Good signs:** \n",
        "    - Row count remains stable\n",
        "    - Appropriate columns are dropped/renamed based on data comparison\n",
        "  - **Warning signs:**\n",
        "    - Row count changes unexpectedly (indicates merge key issues)\n",
        "    - Many columns being renamed when they should be identical (data quality issue)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Placeholder Columns for Future Features\n",
        "\n",
        "Add placeholder columns for FastF1 features that will be extracted later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added placeholder columns:\n",
            "  - lap_time_variance: Variance in lap times (from FastF1 ALL_LAPS)\n",
            "  - throttle_variance: Variance in throttle usage (from FastF1 ALL_TELEMETRY)\n",
            "  - overtake_attempts: Number of overtake attempts (from telemetry analysis)\n",
            "  - avg_pit_stops: Average pit stops per race (from pit_stops.csv aggregation)\n",
            "\n",
            "Master table with placeholders: (12358, 81)\n"
          ]
        }
      ],
      "source": [
        "# Add placeholder columns for future FastF1 features\n",
        "placeholder_features = {\n",
        "    'lap_time_variance': 'Variance in lap times (from FastF1 ALL_LAPS)',\n",
        "    'throttle_variance': 'Variance in throttle usage (from FastF1 ALL_TELEMETRY)',\n",
        "    'overtake_attempts': 'Number of overtake attempts (from telemetry analysis)',\n",
        "    'avg_pit_stops': 'Average pit stops per race (from pit_stops.csv aggregation)'\n",
        "}\n",
        "\n",
        "for col_name, description in placeholder_features.items():\n",
        "    master[col_name] = np.nan\n",
        "\n",
        "print(\"Added placeholder columns:\")\n",
        "for col_name, description in placeholder_features.items():\n",
        "    print(f\"  - {col_name}: {description}\")\n",
        "\n",
        "print(f\"\\nMaster table with placeholders: {master.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Validation\n",
        "\n",
        "Verify data integrity: one row per (raceId, driverId), no unexpected duplicates, date ranges valid.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ No duplicate (raceId, driverId) combinations found\n",
            "\n",
            "Date range: 1994-03-27 00:00:00 to 2024-12-08 00:00:00\n",
            "Years: 1994 - 2024\n",
            "\n",
            "Merge success rates:\n",
            "  driver_standings: 97.5%\n",
            "  constructor_standings: 99.0%\n",
            "  qualifying: 84.9%\n",
            "  sprint_results: 2.9%\n",
            "\n",
            "Final master table: 12,358 rows, 81 columns\n"
          ]
        }
      ],
      "source": [
        "# Verify one row per (raceId, driverId)\n",
        "duplicate_check = master.groupby(['raceId', 'driverId']).size()\n",
        "duplicates = duplicate_check[duplicate_check > 1]\n",
        "if len(duplicates) > 0:\n",
        "    print(f\"WARNING: Found {len(duplicates)} duplicate (raceId, driverId) combinations:\")\n",
        "    print(duplicates.head(10))\n",
        "else:\n",
        "    print(\"✓ No duplicate (raceId, driverId) combinations found\")\n",
        "\n",
        "# Check date ranges\n",
        "if 'date' in master.columns:\n",
        "    master['date'] = pd.to_datetime(master['date'], errors='coerce')\n",
        "    print(f\"\\nDate range: {master['date'].min()} to {master['date'].max()}\")\n",
        "    print(f\"Years: {master['year'].min()} - {master['year'].max()}\")\n",
        "    invalid_dates = master['date'].isnull().sum()\n",
        "    if invalid_dates > 0:\n",
        "        print(f\"WARNING: {invalid_dates} rows with invalid dates\")\n",
        "\n",
        "# Check merge success rates\n",
        "print(f\"\\nMerge success rates:\")\n",
        "if 'driverStandingsId' in master.columns:\n",
        "    driver_standings_coverage = master['driverStandingsId'].notna().sum() / len(master) * 100\n",
        "    print(f\"  driver_standings: {driver_standings_coverage:.1f}%\")\n",
        "if 'constructorStandingsId' in master.columns:\n",
        "    constructor_standings_coverage = master['constructorStandingsId'].notna().sum() / len(master) * 100\n",
        "    print(f\"  constructor_standings: {constructor_standings_coverage:.1f}%\")\n",
        "if 'qualifyId' in master.columns:\n",
        "    qualifying_coverage = master['qualifyId'].notna().sum() / len(master) * 100\n",
        "    print(f\"  qualifying: {qualifying_coverage:.1f}%\")\n",
        "if 'sprint_results_resultId' in master.columns:\n",
        "    sprint_coverage = master['sprint_results_resultId'].notna().sum() / len(master) * 100\n",
        "    print(f\"  sprint_results: {sprint_coverage:.1f}%\")\n",
        "\n",
        "print(f\"\\nFinal master table: {master.shape[0]:,} rows, {master.shape[1]} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Target Variable Creation\n",
        "\n",
        "Create the target variable: podium (1 if positionOrder <= 3, else 0).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Variable Distribution:\n",
            "podium\n",
            "0    10627\n",
            "1     1731\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Podium rate: 14.01%\n",
            "\n",
            "Podium rate by year (sample):\n",
            "      podiums  total  podium_rate\n",
            "year                             \n",
            "2015       57    378     0.150794\n",
            "2016       63    462     0.136364\n",
            "2017       60    400     0.150000\n",
            "2018       63    420     0.150000\n",
            "2019       63    420     0.150000\n",
            "2020       51    340     0.150000\n",
            "2021       66    440     0.150000\n",
            "2022       66    440     0.150000\n",
            "2023       66    440     0.150000\n",
            "2024       72    479     0.150313\n"
          ]
        }
      ],
      "source": [
        "# Create target variable\n",
        "master['podium'] = (master['positionOrder'] <= 3).astype(int)\n",
        "\n",
        "# Verify target distribution\n",
        "print(\"Target Variable Distribution:\")\n",
        "print(master['podium'].value_counts())\n",
        "print(f\"\\nPodium rate: {master['podium'].mean():.2%}\")\n",
        "\n",
        "# Check by year\n",
        "if 'year' in master.columns:\n",
        "    podium_by_year = master.groupby('year')['podium'].agg(['sum', 'count', 'mean'])\n",
        "    podium_by_year.columns = ['podiums', 'total', 'podium_rate']\n",
        "    print(f\"\\nPodium rate by year (sample):\")\n",
        "    print(podium_by_year.tail(10).to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Master Table Export\n",
        "\n",
        "Save the master table and generate schema documentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Master table saved to: C:\\Users\\erikv\\Downloads\\F1\\data\\processed\\master_races.csv\n",
            "  Rows: 12,358\n",
            "  Columns: 82\n",
            "\n",
            "Sample rows:\n",
            "   resultId  raceId  driverId  constructorId number  grid position positionText  positionOrder  points  laps         time milliseconds fastestLap rank fastestLapTime fastestLapSpeed  statusId  year  round  circuitId       date                   name   circuitRef                    circuit_name   location    country      lat      lng  alt                                                        url driverRef number_driver code forename   surname         dob nationality                                   url_driver constructorRef name_constructor nationality_constructor                                               url_constructor  driverStandingsId  driver_standings_points  driver_standings_position driver_standings_positionText  wins  constructorStandingsId  constructor_standings_points  constructor_standings_position constructor_standings_positionText  constructor_standings_wins  constructorResultsId  constructor_results_points status  qualifyId  qualifying_constructorId  qualifying_number  qualifying_position        q1        q2        q3  sprint_results_resultId  sprint_results_constructorId  sprint_results_number  sprint_results_grid sprint_results_position sprint_results_positionText  sprint_results_positionOrder  sprint_results_points  sprint_results_laps sprint_results_time sprint_results_milliseconds sprint_results_fastestLap sprint_results_fastestLapTime  sprint_results_statusId  lap_time_variance  throttle_variance  overtake_attempts  avg_pit_stops  podium\n",
            "0         1      18         1              1     22     1        1            1              1    10.0    58  1:34:50.616      5690616         39    2       1:27.452         218.300         1  2008      1          1 2008-03-16  Australian Grand Prix  albert_park  Albert Park Grand Prix Circuit  Melbourne  Australia -37.8497  144.968   10  http://en.wikipedia.org/wiki/Melbourne_Grand_Prix_Circuit  hamilton            44  HAM    Lewis  Hamilton  1985-01-07     British  http://en.wikipedia.org/wiki/Lewis_Hamilton        mclaren          McLaren                 British                          http://en.wikipedia.org/wiki/McLaren                1.0                     10.0                        1.0                             1   1.0                     1.0                          14.0                             1.0                                  1                         1.0                   1.0                        14.0     \\N        1.0                       1.0               22.0                  1.0  1:26.572  1:25.187  1:26.714                      NaN                           NaN                    NaN                  NaN                     NaN                         NaN                           NaN                    NaN                  NaN                 NaN                         NaN                       NaN                           NaN                      NaN                NaN                NaN                NaN            NaN       1\n",
            "1         2      18         2              2      3     5        2            2              2     8.0    58       +5.478      5696094         41    3       1:27.739         217.586         1  2008      1          1 2008-03-16  Australian Grand Prix  albert_park  Albert Park Grand Prix Circuit  Melbourne  Australia -37.8497  144.968   10  http://en.wikipedia.org/wiki/Melbourne_Grand_Prix_Circuit  heidfeld            \\N  HEI     Nick  Heidfeld  1977-05-10      German   http://en.wikipedia.org/wiki/Nick_Heidfeld     bmw_sauber       BMW Sauber                  German                       http://en.wikipedia.org/wiki/BMW_Sauber                2.0                      8.0                        2.0                             2   0.0                     2.0                           8.0                             3.0                                  3                         0.0                   2.0                         8.0     \\N        5.0                       2.0                3.0                  5.0  1:25.960  1:25.518  1:27.236                      NaN                           NaN                    NaN                  NaN                     NaN                         NaN                           NaN                    NaN                  NaN                 NaN                         NaN                       NaN                           NaN                      NaN                NaN                NaN                NaN            NaN       1\n",
            "2         3      18         3              3      7     7        3            3              3     6.0    58       +8.163      5698779         41    5       1:28.090         216.719         1  2008      1          1 2008-03-16  Australian Grand Prix  albert_park  Albert Park Grand Prix Circuit  Melbourne  Australia -37.8497  144.968   10  http://en.wikipedia.org/wiki/Melbourne_Grand_Prix_Circuit   rosberg             6  ROS     Nico   Rosberg  1985-06-27      German    http://en.wikipedia.org/wiki/Nico_Rosberg       williams         Williams                 British  http://en.wikipedia.org/wiki/Williams_Grand_Prix_Engineering                3.0                      6.0                        3.0                             3   0.0                     3.0                           9.0                             2.0                                  2                         0.0                   3.0                         9.0     \\N        7.0                       3.0                7.0                  7.0  1:26.295  1:26.059  1:28.687                      NaN                           NaN                    NaN                  NaN                     NaN                         NaN                           NaN                    NaN                  NaN                 NaN                         NaN                       NaN                           NaN                      NaN                NaN                NaN                NaN            NaN       1\n",
            "\n",
            "Column summary:\n",
            "  Total columns: 82\n",
            "  Numeric columns: 42\n",
            "  Categorical columns: 39\n",
            "  Date columns: 1\n",
            "\n",
            "Memory usage: 29.15 MB\n"
          ]
        }
      ],
      "source": [
        "# Save master table\n",
        "output_path = PROCESSED_ROOT / \"master_races.csv\"\n",
        "master.to_csv(output_path, index=False)\n",
        "print(f\"Master table saved to: {output_path}\")\n",
        "print(f\"  Rows: {master.shape[0]:,}\")\n",
        "print(f\"  Columns: {master.shape[1]}\")\n",
        "\n",
        "# Display sample rows\n",
        "print(f\"\\nSample rows:\")\n",
        "print(master.head(3).to_string())\n",
        "\n",
        "# Display column summary\n",
        "print(f\"\\nColumn summary:\")\n",
        "print(f\"  Total columns: {len(master.columns)}\")\n",
        "print(f\"  Numeric columns: {len(master.select_dtypes(include=[np.number]).columns)}\")\n",
        "print(f\"  Categorical columns: {len(master.select_dtypes(include=['object']).columns)}\")\n",
        "print(f\"  Date columns: {len(master.select_dtypes(include=['datetime64']).columns)}\")\n",
        "\n",
        "# Memory usage\n",
        "print(f\"\\nMemory usage: {master.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema documentation saved to: C:\\Users\\erikv\\Downloads\\F1\\data\\processed\\master_races_schema.md\n"
          ]
        }
      ],
      "source": [
        "# Generate schema documentation\n",
        "schema_doc = {\n",
        "    'table_name': 'master_races',\n",
        "    'description': 'Combined F1 race data (1994+) with one row per (raceId, driverId)',\n",
        "    'row_count': len(master),\n",
        "    'column_count': len(master.columns),\n",
        "    'columns': {}\n",
        "}\n",
        "\n",
        "# Group columns by source\n",
        "column_sources = {\n",
        "    'results': ['resultId', 'number', 'grid', 'position', 'positionText', 'positionOrder', \n",
        "                'points', 'laps', 'time', 'milliseconds', 'fastestLap', 'rank', \n",
        "                'fastestLapTime', 'fastestLapSpeed', 'statusId'],\n",
        "    'races': ['year', 'round', 'date', 'name'],\n",
        "    'circuits': ['circuit_name', 'location', 'country', 'lat', 'lng', 'alt'],\n",
        "    'drivers': ['driverRef', 'code', 'forename', 'surname', 'dob', 'nationality'],\n",
        "    'constructors': ['constructorRef', 'name', 'nationality'],\n",
        "    'driver_standings': ['driverStandingsId', 'points', 'position', 'positionText', 'wins'],\n",
        "    'constructor_standings': ['constructorStandingsId', 'points', 'position', 'positionText', 'wins'],\n",
        "    'constructor_results': ['constructorResultsId', 'points', 'status'],\n",
        "    'qualifying': ['qualifyId', 'position', 'q1', 'q2', 'q3'],\n",
        "    'sprint_results': [col for col in master.columns if col.startswith('sprint_results_')],\n",
        "    'placeholders': ['lap_time_variance', 'throttle_variance', 'overtake_attempts', 'avg_pit_stops'],\n",
        "    'target': ['podium']\n",
        "}\n",
        "\n",
        "for source, cols in column_sources.items():\n",
        "    for col in cols:\n",
        "        if col in master.columns:\n",
        "            schema_doc['columns'][col] = {\n",
        "                'source': source,\n",
        "                'dtype': str(master[col].dtype),\n",
        "                'null_count': int(master[col].isnull().sum()),\n",
        "                'null_pct': float(master[col].isnull().sum() / len(master) * 100)\n",
        "            }\n",
        "\n",
        "# Save schema\n",
        "import json\n",
        "schema_path = PROCESSED_ROOT / \"master_races_schema.md\"\n",
        "with open(schema_path, 'w') as f:\n",
        "    f.write(\"# Master Races Schema\\n\\n\")\n",
        "    f.write(f\"- **Rows**: {schema_doc['row_count']:,}\\n\")\n",
        "    f.write(f\"- **Columns**: {schema_doc['column_count']}\\n\")\n",
        "    f.write(f\"- **Description**: {schema_doc['description']}\\n\\n\")\n",
        "    f.write(\"## Column Sources\\n\\n\")\n",
        "    for source, cols in column_sources.items():\n",
        "        actual_cols = [c for c in cols if c in master.columns]\n",
        "        if actual_cols:\n",
        "            f.write(f\"### {source}\\n\")\n",
        "            for col in actual_cols[:10]:  # Show first 10\n",
        "                info = schema_doc['columns'][col]\n",
        "                f.write(f\"- `{col}` ({info['dtype']}): {info['null_pct']:.1f}% null\\n\")\n",
        "            if len(actual_cols) > 10:\n",
        "                f.write(f\"- ... and {len(actual_cols) - 10} more\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"Schema documentation saved to: {schema_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
