{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Modeling & Evaluation\n",
        "\n",
        "Train multiple models (LightGBM, CatBoost, TabNet) to predict podium probability (top-3 finish).\n",
        "\n",
        "**Models:**\n",
        "1. **LightGBM**: Primary tree-based ensemble with categorical feature support\n",
        "2. **CatBoost**: Alternative tree ensemble with built-in categorical handling\n",
        "3. **TabNet**: Deep learning model with attention-based feature selection\n",
        "\n",
        "**Train/Val/Test Split:**\n",
        "- Train: 1994-2022\n",
        "- Validation: 2023\n",
        "- Test: 2024\n",
        "\n",
        "**Input:** `data/processed/master_races_clean.csv` (from 03.8)  \n",
        "**Output:** `models/best_podium_model.pkl`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded features: (12358, 54)\n",
            "Date range: 1994-03-27 00:00:00 to 2024-12-08 00:00:00\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    brier_score_loss,\n",
        "    log_loss,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Models\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "    print(\"Warning: LightGBM not available\")\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CATBOOST_AVAILABLE = False\n",
        "    print(\"Warning: CatBoost not available\")\n",
        "\n",
        "try:\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "    TABNET_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TABNET_AVAILABLE = False\n",
        "    print(\"Warning: TabNet not available\")\n",
        "\n",
        "# Set up paths\n",
        "# Get project root (works whether running from notebooks/ or F1/ folder)\n",
        "PROJECT_ROOT = Path().resolve()\n",
        "if PROJECT_ROOT.name == 'notebooks':\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "PROCESSED_ROOT = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "MODELS_ROOT = PROJECT_ROOT / \"models\"\n",
        "MODELS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load features from cleaned dataset (output from 03.8)\n",
        "features = pd.read_csv(PROCESSED_ROOT / \"master_races_clean.csv\")\n",
        "features['date'] = pd.to_datetime(features['date'], errors='coerce')\n",
        "\n",
        "print(f\"Loaded features: {features.shape}\")\n",
        "print(f\"Date range: {features['date'].min()} to {features['date'].max()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "Prepare train/val/test splits and handle missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Excluded 18 outcome/metadata features\n",
            "Remaining features: 36\n",
            "Features shape: (12358, 36)\n",
            "Target distribution: {0: 10627, 1: 1731}\n",
            "Podium rate: 14.01%\n",
            "\n",
            "Train: 11,439 samples (1994-2022)\n",
            "Val: 440 samples (2023)\n",
            "Test: 479 samples (2024)\n"
          ]
        }
      ],
      "source": [
        "# Prepare features and target\n",
        "# EXCLUDE race outcome features (known only after race - would cause data leakage)\n",
        "outcome_features = [\n",
        "    # Race results (outcomes)\n",
        "    'points',                      # Race points (outcome)\n",
        "    'position',                    # Final race position (outcome)\n",
        "    'positionOrder',               # Final race position order (outcome)\n",
        "    'milliseconds',                # Race finish time in milliseconds (outcome)\n",
        "    'time',                        # Race finish time (outcome)\n",
        "    'laps',                        # Number of laps completed (outcome)\n",
        "    'fastestLap',                  # Fastest lap number (outcome)\n",
        "    'fastestLapTime',              # Fastest lap time (outcome)\n",
        "    'fastestLapSpeed',             # Fastest lap speed (outcome)\n",
        "    'rank',                        # Fastest lap rank (outcome)\n",
        "    'statusId',                    # Race finish status ID (outcome)\n",
        "    'status_category',             # Race finish status category (outcome)\n",
        "    'resultId',                    # Result ID (outcome identifier)\n",
        "    'constructor_results_points',  # Constructor race points (outcome)\n",
        "    # Sprint race results (outcomes)\n",
        "    #'sprint_results_positionOrder',    # Sprint race position (outcome)\n",
        "    #'sprint_results_points',          # Sprint race points (outcome)\n",
        "    #'sprint_results_time',            # Sprint race time (outcome)\n",
        "    #'sprint_results_milliseconds',    # Sprint race time in milliseconds (outcome)\n",
        "    #'sprint_results_fastestLap',      # Sprint fastest lap (outcome)\n",
        "    #'sprint_results_fastestLapTime',  # Sprint fastest lap time (outcome)\n",
        "    #'sprint_results_laps',            # Sprint laps completed (outcome)\n",
        "    #'sprint_results_statusId',         # Sprint finish status (outcome)\n",
        "    # Non-PRE_RACE standings (include current race - data leakage)\n",
        "    'driver_standings_points',        # Includes current race (use PRE_RACE version instead)\n",
        "    'driver_standings_position',      # Includes current race (use PRE_RACE version instead)\n",
        "    'constructor_standings_points',   # Includes current race (use PRE_RACE version instead)\n",
        "    'constructor_standings_position', # Includes current race (use PRE_RACE version instead)\n",
        "]\n",
        "\n",
        "# Metadata and target to exclude\n",
        "exclude_cols = ['podium', 'raceId', 'driverId', 'date', 'year'] + outcome_features\n",
        "\n",
        "# Remove outcome features and metadata\n",
        "X = features.drop(columns=exclude_cols, errors='ignore')\n",
        "y = features['podium']\n",
        "\n",
        "print(f\"Excluded {len([c for c in exclude_cols if c in features.columns])} outcome/metadata features\")\n",
        "print(f\"Remaining features: {X.shape[1]}\")\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str).fillna('Unknown'))\n",
        "\n",
        "# Fill missing values with median for numeric, mode for categorical\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in [np.float64, np.int64]:\n",
        "        X[col] = X[col].fillna(X[col].median())\n",
        "    else:\n",
        "        X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "print(f\"Podium rate: {y.mean():.2%}\")\n",
        "\n",
        "# Train/Val/Test split\n",
        "train_mask = features['year'] < 2023\n",
        "val_mask = features['year'] == 2023\n",
        "test_mask = features['year'] == 2024\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_val, y_val = X[val_mask], y[val_mask]\n",
        "X_test, y_test = X[test_mask], y[test_mask]\n",
        "\n",
        "print(f\"\\nTrain: {len(X_train):,} samples ({features[train_mask]['year'].min()}-{features[train_mask]['year'].max()})\")\n",
        "print(f\"Val: {len(X_val):,} samples (2023)\")\n",
        "print(f\"Test: {len(X_test):,} samples (2024)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model 1: LightGBM\n",
        "\n",
        "Train LightGBM with categorical feature support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[149]\tvalid_0's binary_logloss: 0.175533\n",
            "LightGBM Results:\n",
            "  Val ROC-AUC: 0.9591\n",
            "  Val PR-AUC: 0.8400\n",
            "  Test ROC-AUC: 0.9641\n",
            "  Test PR-AUC: 0.8306\n",
            "\n",
            "Top 10 Features (LightGBM):\n",
            "                           feature  importance\n",
            "         driver_points_avg_last_10         404\n",
            "            driver_avg_grid_last_5         382\n",
            "            driver_races_completed         333\n",
            "              driver_total_podiums         305\n",
            "                              grid         298\n",
            "        driver_avg_position_last_5         261\n",
            "   constructor_podium_rate_last_15         259\n",
            "constructor_podium_rate_at_circuit         252\n",
            "                        driver_age         210\n",
            "  driver_standings_points_PRE_RACE         203\n"
          ]
        }
      ],
      "source": [
        "if LIGHTGBM_AVAILABLE:\n",
        "    # LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=7,\n",
        "        num_leaves=31,\n",
        "        objective='binary',\n",
        "        metric='binary_logloss',\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    )\n",
        "    \n",
        "    lgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
        "    )\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
        "    y_test_pred_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    lgb_val_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_val, y_val_pred_lgb),\n",
        "        'pr_auc': average_precision_score(y_val, y_val_pred_lgb),\n",
        "        'brier': brier_score_loss(y_val, y_val_pred_lgb),\n",
        "        'log_loss': log_loss(y_val, y_val_pred_lgb)\n",
        "    }\n",
        "    \n",
        "    lgb_test_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_test, y_test_pred_lgb),\n",
        "        'pr_auc': average_precision_score(y_test, y_test_pred_lgb),\n",
        "        'brier': brier_score_loss(y_test, y_test_pred_lgb),\n",
        "        'log_loss': log_loss(y_test, y_test_pred_lgb)\n",
        "    }\n",
        "    \n",
        "    print(\"LightGBM Results:\")\n",
        "    print(f\"  Val ROC-AUC: {lgb_val_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Val PR-AUC: {lgb_val_metrics['pr_auc']:.4f}\")\n",
        "    print(f\"  Test ROC-AUC: {lgb_test_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Test PR-AUC: {lgb_test_metrics['pr_auc']:.4f}\")\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance_lgb = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': lgb_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTop 10 Features (LightGBM):\")\n",
        "    print(feature_importance_lgb.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"LightGBM not available - skipping\")\n",
        "    lgb_model = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model 2: CatBoost\n",
        "\n",
        "Train CatBoost with built-in categorical handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatBoost Results:\n",
            "  Val ROC-AUC: 0.9570\n",
            "  Val PR-AUC: 0.8192\n",
            "  Test ROC-AUC: 0.9622\n",
            "  Test PR-AUC: 0.8318\n",
            "\n",
            "Top 10 Features (CatBoost):\n",
            "                                feature  importance\n",
            "                                   grid   13.415982\n",
            "        constructor_podium_rate_last_15   10.313532\n",
            "              driver_points_avg_last_10    9.914227\n",
            "                 driver_avg_grid_last_5    7.781556\n",
            "                 driver_races_completed    7.450418\n",
            "                   driver_total_podiums    7.289335\n",
            "constructor_standings_position_PRE_RACE    6.810534\n",
            "     driver_standings_position_PRE_RACE    5.281715\n",
            "             driver_avg_position_last_5    4.453291\n",
            "     constructor_podium_rate_at_circuit    4.222331\n"
          ]
        }
      ],
      "source": [
        "if CATBOOST_AVAILABLE:\n",
        "    # CatBoost model\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=500,\n",
        "        learning_rate=0.05,\n",
        "        depth=7,\n",
        "        loss_function='Logloss',\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    cat_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        early_stopping_rounds=50\n",
        "    )\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred_cat = cat_model.predict_proba(X_val)[:, 1]\n",
        "    y_test_pred_cat = cat_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    cat_val_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_val, y_val_pred_cat),\n",
        "        'pr_auc': average_precision_score(y_val, y_val_pred_cat),\n",
        "        'brier': brier_score_loss(y_val, y_val_pred_cat),\n",
        "        'log_loss': log_loss(y_val, y_val_pred_cat)\n",
        "    }\n",
        "    \n",
        "    cat_test_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_test, y_test_pred_cat),\n",
        "        'pr_auc': average_precision_score(y_test, y_test_pred_cat),\n",
        "        'brier': brier_score_loss(y_test, y_test_pred_cat),\n",
        "        'log_loss': log_loss(y_test, y_test_pred_cat)\n",
        "    }\n",
        "    \n",
        "    print(\"CatBoost Results:\")\n",
        "    print(f\"  Val ROC-AUC: {cat_val_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Val PR-AUC: {cat_val_metrics['pr_auc']:.4f}\")\n",
        "    print(f\"  Test ROC-AUC: {cat_test_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Test PR-AUC: {cat_test_metrics['pr_auc']:.4f}\")\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance_cat = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': cat_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTop 10 Features (CatBoost):\")\n",
        "    print(feature_importance_cat.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(\"CatBoost not available - skipping\")\n",
        "    cat_model = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model 3: TabNet\n",
        "\n",
        "Train TabNet deep learning model with attention-based feature selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.58362 | val_0_auc: 0.6665  |  0:00:02s\n",
            "epoch 1  | loss: 0.3486  | val_0_auc: 0.63227 |  0:00:04s\n",
            "epoch 2  | loss: 0.30232 | val_0_auc: 0.65285 |  0:00:07s\n",
            "epoch 3  | loss: 0.28028 | val_0_auc: 0.71488 |  0:00:09s\n",
            "epoch 4  | loss: 0.27022 | val_0_auc: 0.77564 |  0:00:11s\n",
            "epoch 5  | loss: 0.25297 | val_0_auc: 0.7931  |  0:00:13s\n",
            "epoch 6  | loss: 0.24855 | val_0_auc: 0.84654 |  0:00:16s\n",
            "epoch 7  | loss: 0.23939 | val_0_auc: 0.829   |  0:00:18s\n",
            "epoch 8  | loss: 0.24126 | val_0_auc: 0.82527 |  0:00:20s\n",
            "epoch 9  | loss: 0.23681 | val_0_auc: 0.85132 |  0:00:22s\n",
            "epoch 10 | loss: 0.23241 | val_0_auc: 0.84152 |  0:00:24s\n",
            "epoch 11 | loss: 0.23368 | val_0_auc: 0.85602 |  0:00:26s\n",
            "epoch 12 | loss: 0.22602 | val_0_auc: 0.85841 |  0:00:29s\n",
            "epoch 13 | loss: 0.22427 | val_0_auc: 0.86339 |  0:00:31s\n",
            "epoch 14 | loss: 0.22454 | val_0_auc: 0.85549 |  0:00:33s\n",
            "epoch 15 | loss: 0.21909 | val_0_auc: 0.86939 |  0:00:35s\n",
            "epoch 16 | loss: 0.22173 | val_0_auc: 0.89827 |  0:00:37s\n",
            "epoch 17 | loss: 0.21797 | val_0_auc: 0.87032 |  0:00:38s\n",
            "epoch 18 | loss: 0.21896 | val_0_auc: 0.89046 |  0:00:40s\n",
            "epoch 19 | loss: 0.21823 | val_0_auc: 0.87786 |  0:00:42s\n",
            "epoch 20 | loss: 0.21877 | val_0_auc: 0.83799 |  0:00:44s\n",
            "epoch 21 | loss: 0.22174 | val_0_auc: 0.79793 |  0:00:46s\n",
            "epoch 22 | loss: 0.21684 | val_0_auc: 0.86542 |  0:00:49s\n",
            "epoch 23 | loss: 0.21072 | val_0_auc: 0.87214 |  0:00:51s\n",
            "epoch 24 | loss: 0.21492 | val_0_auc: 0.87085 |  0:00:53s\n",
            "epoch 25 | loss: 0.2162  | val_0_auc: 0.86307 |  0:00:56s\n",
            "epoch 26 | loss: 0.21642 | val_0_auc: 0.86072 |  0:00:58s\n",
            "epoch 27 | loss: 0.21444 | val_0_auc: 0.87656 |  0:01:00s\n",
            "epoch 28 | loss: 0.2138  | val_0_auc: 0.81705 |  0:01:02s\n",
            "epoch 29 | loss: 0.21245 | val_0_auc: 0.83552 |  0:01:04s\n",
            "epoch 30 | loss: 0.20865 | val_0_auc: 0.84966 |  0:01:07s\n",
            "epoch 31 | loss: 0.20947 | val_0_auc: 0.89479 |  0:01:09s\n",
            "epoch 32 | loss: 0.21235 | val_0_auc: 0.89434 |  0:01:11s\n",
            "epoch 33 | loss: 0.20805 | val_0_auc: 0.90285 |  0:01:13s\n",
            "epoch 34 | loss: 0.20555 | val_0_auc: 0.89515 |  0:01:16s\n",
            "epoch 35 | loss: 0.2018  | val_0_auc: 0.90038 |  0:01:18s\n",
            "epoch 36 | loss: 0.20351 | val_0_auc: 0.90747 |  0:01:20s\n",
            "epoch 37 | loss: 0.20266 | val_0_auc: 0.90451 |  0:01:22s\n",
            "epoch 38 | loss: 0.20598 | val_0_auc: 0.91087 |  0:01:25s\n",
            "epoch 39 | loss: 0.2021  | val_0_auc: 0.90046 |  0:01:27s\n",
            "epoch 40 | loss: 0.19984 | val_0_auc: 0.90233 |  0:01:29s\n",
            "epoch 41 | loss: 0.19737 | val_0_auc: 0.90399 |  0:01:31s\n",
            "epoch 42 | loss: 0.19709 | val_0_auc: 0.89471 |  0:01:33s\n",
            "epoch 43 | loss: 0.20712 | val_0_auc: 0.88681 |  0:01:35s\n",
            "epoch 44 | loss: 0.21171 | val_0_auc: 0.87502 |  0:01:38s\n",
            "epoch 45 | loss: 0.20956 | val_0_auc: 0.88661 |  0:01:40s\n",
            "epoch 46 | loss: 0.2017  | val_0_auc: 0.89568 |  0:01:42s\n",
            "epoch 47 | loss: 0.20822 | val_0_auc: 0.8984  |  0:01:44s\n",
            "epoch 48 | loss: 0.20676 | val_0_auc: 0.84763 |  0:01:46s\n",
            "epoch 49 | loss: 0.20639 | val_0_auc: 0.84257 |  0:01:49s\n",
            "epoch 50 | loss: 0.20149 | val_0_auc: 0.89848 |  0:01:51s\n",
            "epoch 51 | loss: 0.19819 | val_0_auc: 0.91821 |  0:01:53s\n",
            "epoch 52 | loss: 0.19949 | val_0_auc: 0.89297 |  0:01:55s\n",
            "epoch 53 | loss: 0.19631 | val_0_auc: 0.87842 |  0:01:58s\n",
            "epoch 54 | loss: 0.19945 | val_0_auc: 0.89746 |  0:02:00s\n",
            "epoch 55 | loss: 0.19734 | val_0_auc: 0.90233 |  0:02:02s\n",
            "epoch 56 | loss: 0.19879 | val_0_auc: 0.89876 |  0:02:04s\n",
            "epoch 57 | loss: 0.20078 | val_0_auc: 0.89682 |  0:02:07s\n",
            "epoch 58 | loss: 0.19619 | val_0_auc: 0.89037 |  0:02:09s\n",
            "epoch 59 | loss: 0.19216 | val_0_auc: 0.90735 |  0:02:12s\n",
            "epoch 60 | loss: 0.19267 | val_0_auc: 0.90305 |  0:02:14s\n",
            "epoch 61 | loss: 0.19504 | val_0_auc: 0.89487 |  0:02:17s\n",
            "epoch 62 | loss: 0.19174 | val_0_auc: 0.89977 |  0:02:19s\n",
            "epoch 63 | loss: 0.19839 | val_0_auc: 0.88507 |  0:02:21s\n",
            "epoch 64 | loss: 0.1978  | val_0_auc: 0.91817 |  0:02:24s\n",
            "epoch 65 | loss: 0.196   | val_0_auc: 0.89528 |  0:02:26s\n",
            "epoch 66 | loss: 0.19298 | val_0_auc: 0.9148  |  0:02:28s\n",
            "epoch 67 | loss: 0.18791 | val_0_auc: 0.90378 |  0:02:31s\n",
            "epoch 68 | loss: 0.19549 | val_0_auc: 0.89941 |  0:02:33s\n",
            "epoch 69 | loss: 0.19776 | val_0_auc: 0.92214 |  0:02:36s\n",
            "epoch 70 | loss: 0.19689 | val_0_auc: 0.90674 |  0:02:38s\n",
            "epoch 71 | loss: 0.19393 | val_0_auc: 0.86894 |  0:02:40s\n",
            "epoch 72 | loss: 0.19315 | val_0_auc: 0.8689  |  0:02:43s\n",
            "epoch 73 | loss: 0.18866 | val_0_auc: 0.88369 |  0:02:45s\n",
            "epoch 74 | loss: 0.1898  | val_0_auc: 0.83755 |  0:02:48s\n",
            "epoch 75 | loss: 0.20961 | val_0_auc: 0.86542 |  0:02:50s\n",
            "epoch 76 | loss: 0.20688 | val_0_auc: 0.86064 |  0:02:53s\n",
            "epoch 77 | loss: 0.20139 | val_0_auc: 0.8971  |  0:02:55s\n",
            "epoch 78 | loss: 0.19655 | val_0_auc: 0.89471 |  0:02:58s\n",
            "epoch 79 | loss: 0.19867 | val_0_auc: 0.91318 |  0:03:01s\n",
            "epoch 80 | loss: 0.19455 | val_0_auc: 0.89884 |  0:03:04s\n",
            "epoch 81 | loss: 0.19009 | val_0_auc: 0.9191  |  0:03:07s\n",
            "epoch 82 | loss: 0.1928  | val_0_auc: 0.90184 |  0:03:11s\n",
            "epoch 83 | loss: 0.19045 | val_0_auc: 0.89852 |  0:03:14s\n",
            "epoch 84 | loss: 0.18925 | val_0_auc: 0.92979 |  0:03:17s\n",
            "epoch 85 | loss: 0.18759 | val_0_auc: 0.92424 |  0:03:19s\n",
            "epoch 86 | loss: 0.18486 | val_0_auc: 0.92607 |  0:03:22s\n",
            "epoch 87 | loss: 0.18489 | val_0_auc: 0.93526 |  0:03:25s\n",
            "epoch 88 | loss: 0.18081 | val_0_auc: 0.93348 |  0:03:27s\n",
            "epoch 89 | loss: 0.18499 | val_0_auc: 0.93688 |  0:03:30s\n",
            "epoch 90 | loss: 0.17948 | val_0_auc: 0.93737 |  0:03:33s\n",
            "epoch 91 | loss: 0.17782 | val_0_auc: 0.93595 |  0:03:35s\n",
            "epoch 92 | loss: 0.17656 | val_0_auc: 0.93295 |  0:03:38s\n",
            "epoch 93 | loss: 0.17131 | val_0_auc: 0.93348 |  0:03:41s\n",
            "epoch 94 | loss: 0.17038 | val_0_auc: 0.93781 |  0:03:44s\n",
            "epoch 95 | loss: 0.17487 | val_0_auc: 0.93684 |  0:03:47s\n",
            "epoch 96 | loss: 0.17506 | val_0_auc: 0.92424 |  0:03:49s\n",
            "epoch 97 | loss: 0.1733  | val_0_auc: 0.91776 |  0:03:52s\n",
            "epoch 98 | loss: 0.1745  | val_0_auc: 0.91983 |  0:03:54s\n",
            "epoch 99 | loss: 0.16771 | val_0_auc: 0.93072 |  0:03:57s\n",
            "Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_val_0_auc = 0.93781\n",
            "TabNet Results:\n",
            "  Val ROC-AUC: 0.9378\n",
            "  Val PR-AUC: 0.7787\n",
            "  Test ROC-AUC: 0.9316\n",
            "  Test PR-AUC: 0.6994\n"
          ]
        }
      ],
      "source": [
        "if TABNET_AVAILABLE:\n",
        "    import torch\n",
        "    # TabNet model\n",
        "    tabnet_model = TabNetClassifier(\n",
        "        n_d=64,\n",
        "        n_a=64,\n",
        "        n_steps=5,\n",
        "        gamma=1.5,\n",
        "        n_independent=2,\n",
        "        n_shared=2,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=2e-2),\n",
        "        scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        mask_type='entmax',\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    tabnet_model.fit(\n",
        "        X_train.values, y_train.values,\n",
        "        eval_set=[(X_val.values, y_val.values)],\n",
        "        eval_metric=['auc'],\n",
        "        max_epochs=100,\n",
        "        patience=20,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128\n",
        "    )\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred_tab = tabnet_model.predict_proba(X_val.values)[:, 1]\n",
        "    y_test_pred_tab = tabnet_model.predict_proba(X_test.values)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    tabnet_val_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_val, y_val_pred_tab),\n",
        "        'pr_auc': average_precision_score(y_val, y_val_pred_tab),\n",
        "        'brier': brier_score_loss(y_val, y_val_pred_tab),\n",
        "        'log_loss': log_loss(y_val, y_val_pred_tab)\n",
        "    }\n",
        "    \n",
        "    tabnet_test_metrics = {\n",
        "        'roc_auc': roc_auc_score(y_test, y_test_pred_tab),\n",
        "        'pr_auc': average_precision_score(y_test, y_test_pred_tab),\n",
        "        'brier': brier_score_loss(y_test, y_test_pred_tab),\n",
        "        'log_loss': log_loss(y_test, y_test_pred_tab)\n",
        "    }\n",
        "    \n",
        "    print(\"TabNet Results:\")\n",
        "    print(f\"  Val ROC-AUC: {tabnet_val_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Val PR-AUC: {tabnet_val_metrics['pr_auc']:.4f}\")\n",
        "    print(f\"  Test ROC-AUC: {tabnet_test_metrics['roc_auc']:.4f}\")\n",
        "    print(f\"  Test PR-AUC: {tabnet_test_metrics['pr_auc']:.4f}\")\n",
        "else:\n",
        "    print(\"TabNet not available - skipping\")\n",
        "    tabnet_model = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison\n",
        "\n",
        "Compare all models and select the best one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Comparison (Validation Set):\n",
            "   model  val_roc_auc  val_pr_auc\n",
            "LightGBM     0.959123    0.840048\n",
            "CatBoost     0.957017    0.819187\n",
            "  TabNet     0.937814    0.778719\n",
            "\n",
            "Best model (by Val PR-AUC): LightGBM\n"
          ]
        }
      ],
      "source": [
        "# Compare models on validation set\n",
        "model_comparison = []\n",
        "\n",
        "if LIGHTGBM_AVAILABLE:\n",
        "    model_comparison.append({\n",
        "        'model': 'LightGBM',\n",
        "        'val_roc_auc': lgb_val_metrics['roc_auc'],\n",
        "        'val_pr_auc': lgb_val_metrics['pr_auc'],\n",
        "        'test_roc_auc': lgb_test_metrics['roc_auc'],\n",
        "        'test_pr_auc': lgb_test_metrics['pr_auc']\n",
        "    })\n",
        "\n",
        "if CATBOOST_AVAILABLE:\n",
        "    model_comparison.append({\n",
        "        'model': 'CatBoost',\n",
        "        'val_roc_auc': cat_val_metrics['roc_auc'],\n",
        "        'val_pr_auc': cat_val_metrics['pr_auc'],\n",
        "        'test_roc_auc': cat_test_metrics['roc_auc'],\n",
        "        'test_pr_auc': cat_test_metrics['pr_auc']\n",
        "    })\n",
        "\n",
        "if TABNET_AVAILABLE:\n",
        "    model_comparison.append({\n",
        "        'model': 'TabNet',\n",
        "        'val_roc_auc': tabnet_val_metrics['roc_auc'],\n",
        "        'val_pr_auc': tabnet_val_metrics['pr_auc'],\n",
        "        'test_roc_auc': tabnet_test_metrics['roc_auc'],\n",
        "        'test_pr_auc': tabnet_test_metrics['pr_auc']\n",
        "    })\n",
        "\n",
        "if model_comparison:\n",
        "    comparison_df = pd.DataFrame(model_comparison)\n",
        "    print(\"Model Comparison (Validation Set):\")\n",
        "    print(comparison_df[['model', 'val_roc_auc', 'val_pr_auc']].to_string(index=False))\n",
        "    \n",
        "    # Select best model based on validation PR-AUC\n",
        "    best_model_name = comparison_df.loc[comparison_df['val_pr_auc'].idxmax(), 'model']\n",
        "    print(f\"\\nBest model (by Val PR-AUC): {best_model_name}\")\n",
        "    \n",
        "    # Get best model\n",
        "    if best_model_name == 'LightGBM' and LIGHTGBM_AVAILABLE:\n",
        "        best_model = lgb_model\n",
        "    elif best_model_name == 'CatBoost' and CATBOOST_AVAILABLE:\n",
        "        best_model = cat_model\n",
        "    elif best_model_name == 'TabNet' and TABNET_AVAILABLE:\n",
        "        best_model = tabnet_model\n",
        "    else:\n",
        "        best_model = None\n",
        "else:\n",
        "    print(\"No models available for comparison\")\n",
        "    best_model = None\n",
        "    best_model_name = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Persistence\n",
        "\n",
        "Save the best model for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model saved to: C:\\Users\\erikv\\Downloads\\models\\best_podium_model.pkl\n",
            "Model comparison saved to: C:\\Users\\erikv\\Downloads\\F1\\data\\processed\\model_comparison.csv\n"
          ]
        }
      ],
      "source": [
        "if best_model is not None:\n",
        "    import joblib\n",
        "    \n",
        "    # Save best model\n",
        "    model_output_path = MODELS_ROOT / \"best_podium_model.pkl\"\n",
        "    joblib.dump(best_model, model_output_path)\n",
        "    print(f\"Best model saved to: {model_output_path}\")\n",
        "    \n",
        "    # Save comparison results\n",
        "    if model_comparison:\n",
        "        comparison_df.to_csv(PROCESSED_ROOT / \"model_comparison.csv\", index=False)\n",
        "        print(f\"Model comparison saved to: {PROCESSED_ROOT / 'model_comparison.csv'}\")\n",
        "else:\n",
        "    print(\"No model to save\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Trained LightGBM, CatBoost, and TabNet models on 1994-2022 data\n",
        "- Validated on 2023 data, tested on 2024 data\n",
        "- Selected best model based on validation PR-AUC\n",
        "- Saved best model to `models/best_podium_model.pkl` for downstream inference\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
