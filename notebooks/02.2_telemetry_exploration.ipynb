{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02.2 - Telemetry Data Exploration (Memory-Efficient)\n",
        "\n",
        "This notebook provides **resource-efficient** exploration of the TELEMETRY data, which is extremely large and RAM-intensive.\n",
        "\n",
        "## Memory-Efficient Techniques Used:\n",
        "1. **Chunked Reading**: Process data in small chunks instead of loading entire files\n",
        "2. **Sampling**: Examine random samples from large files\n",
        "3. **Selective Column Reading**: Only load columns we need to inspect\n",
        "4. **Memory Profiling**: Track memory usage during operations\n",
        "5. **Incremental Processing**: Process one file/year at a time and clear memory between operations\n",
        "\n",
        "## Goals:\n",
        "- Understand the structure of TELEMETRY data\n",
        "- Identify available columns and data types\n",
        "- Check data volume and memory requirements\n",
        "- Prepare for feature extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastF1 data: C:\\Users\\erikv\\Downloads\\F1\\data\\raw\\fastf1_2018plus\n",
            "Python version: 3.12.9 (tags/v3.12.9:fdb8142, Feb  4 2025, 15:27:58) [MSC v.1942 64 bit (AMD64)]\n",
            "Pandas version: 2.3.3\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Memory profiling (optional - install with: pip install memory-profiler)\n",
        "try:\n",
        "    from memory_profiler import profile\n",
        "    MEMORY_PROFILING = True\n",
        "except ImportError:\n",
        "    MEMORY_PROFILING = False\n",
        "    print(\"Note: memory-profiler not installed. Install with: pip install memory-profiler\")\n",
        "\n",
        "# Set up paths\n",
        "# Get project root (works whether running from notebooks/ or F1/ folder)\n",
        "PROJECT_ROOT = Path().resolve()\n",
        "if PROJECT_ROOT.name == 'notebooks':\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "FASTF1_ROOT = PROJECT_ROOT / \"data\" / \"raw\" / \"fastf1_2018plus\"\n",
        "\n",
        "print(f\"FastF1 data: {FASTF1_ROOT}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. File Size Analysis\n",
        "Check the size of all TELEMETRY files to understand data volume.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TELEMETRY File Sizes:\n",
            "============================================================\n",
            "ALL_TELEMETRY_2018.csv            2398.98 MB ( 2.343 GB)\n",
            "ALL_TELEMETRY_2019.csv            6466.11 MB ( 6.315 GB)\n",
            "ALL_TELEMETRY_2020.csv            2060.52 MB ( 2.012 GB)\n",
            "ALL_TELEMETRY_2021.csv            6636.65 MB ( 6.481 GB)\n",
            "ALL_TELEMETRY_2022.csv             214.53 MB ( 0.210 GB)\n",
            "ALL_TELEMETRY_2023.csv            5905.84 MB ( 5.767 GB)\n",
            "ALL_TELEMETRY_2024.csv            6205.47 MB ( 6.060 GB)\n",
            "============================================================\n",
            "Total                            29888.11 MB (29.188 GB)\n",
            "\n",
            "Number of files: 7\n"
          ]
        }
      ],
      "source": [
        "# Check file sizes for all telemetry files\n",
        "telemetry_files = sorted(FASTF1_ROOT.glob(\"ALL_TELEMETRY_*.csv\"))\n",
        "\n",
        "print(\"TELEMETRY File Sizes:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "total_size = 0\n",
        "for file_path in telemetry_files:\n",
        "    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "    size_gb = size_mb / 1024\n",
        "    total_size += size_mb\n",
        "    \n",
        "    # Get approximate line count (first line is header, estimate from file size)\n",
        "    # This is a rough estimate - actual count will be verified in chunks\n",
        "    print(f\"{file_path.name:30s} {size_mb:>10.2f} MB ({size_gb:>6.3f} GB)\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Total':30s} {total_size:>10.2f} MB ({total_size/1024:>6.3f} GB)\")\n",
        "print(f\"\\nNumber of files: {len(telemetry_files)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Memory-Efficient Column Discovery\n",
        "Read only the first chunk to discover columns without loading the entire file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing structure of: ALL_TELEMETRY_2018.csv\n",
            "============================================================\n",
            "\n",
            "Columns (14):\n",
            "   1. Date                           object\n",
            "   2. RPM                            float64\n",
            "   3. Speed                          float64\n",
            "   4. nGear                          int64\n",
            "   5. Throttle                       float64\n",
            "   6. Brake                          bool\n",
            "   7. DRS                            int64\n",
            "   8. Source                         object\n",
            "   9. Time                           object\n",
            "  10. SessionTime                    object\n",
            "  11. Year                           int64\n",
            "  12. Event                          object\n",
            "  13. Session                        object\n",
            "  14. Driver                         int64\n",
            "\n",
            "First chunk memory usage: 0.42 MB\n",
            "\n",
            "Sample row (first record):\n",
            "  Date                          : 2018-03-25 05:06:03.659\n",
            "  RPM                           : 0.0\n",
            "  Speed                         : 0.0\n",
            "  nGear                         : 0\n",
            "  Throttle                      : 0.0\n",
            "  Brake                         : False\n",
            "  DRS                           : 1\n",
            "  Source                        : car\n",
            "  Time                          : -1 days +23:59:52.478000\n",
            "  SessionTime                   : -1 days +23:59:52.478000\n",
            "  ... and 4 more columns\n"
          ]
        }
      ],
      "source": [
        "# Function to get column info from first chunk\n",
        "def get_columns_info(file_path, chunk_size=1000):\n",
        "    \"\"\"Read first chunk to discover columns and data types\"\"\"\n",
        "    try:\n",
        "        # Read only first chunk\n",
        "        chunk_iter = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)\n",
        "        first_chunk = next(chunk_iter)\n",
        "        \n",
        "        info = {\n",
        "            'columns': list(first_chunk.columns),\n",
        "            'n_columns': len(first_chunk.columns),\n",
        "            'dtypes': first_chunk.dtypes.to_dict(),\n",
        "            'sample_row': first_chunk.iloc[0].to_dict() if len(first_chunk) > 0 else None,\n",
        "            'memory_usage_mb': first_chunk.memory_usage(deep=True).sum() / (1024 * 1024)\n",
        "        }\n",
        "        \n",
        "        return info, first_chunk\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path.name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Check first file (or smallest) for structure\n",
        "test_file = telemetry_files[0] if telemetry_files else None\n",
        "\n",
        "if test_file:\n",
        "    print(f\"Analyzing structure of: {test_file.name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    info, sample_df = get_columns_info(test_file, chunk_size=1000)\n",
        "    \n",
        "    if info:\n",
        "        print(f\"\\nColumns ({info['n_columns']}):\")\n",
        "        for i, col in enumerate(info['columns'], 1):\n",
        "            dtype = info['dtypes'][col]\n",
        "            print(f\"  {i:2d}. {col:30s} {str(dtype)}\")\n",
        "        \n",
        "        print(f\"\\nFirst chunk memory usage: {info['memory_usage_mb']:.2f} MB\")\n",
        "        \n",
        "        print(f\"\\nSample row (first record):\")\n",
        "        if info['sample_row']:\n",
        "            for key, value in list(info['sample_row'].items())[:10]:  # First 10 columns\n",
        "                print(f\"  {key:30s}: {value}\")\n",
        "            if len(info['sample_row']) > 10:\n",
        "                print(f\"  ... and {len(info['sample_row']) - 10} more columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Efficient Row Count Estimation\n",
        "Count rows without loading entire file into memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counting rows in: ALL_TELEMETRY_2018.csv\n",
            "This may take a while for large files...\n",
            "Total rows: 19,140,560\n",
            "Chunks processed: 1915\n",
            "Estimated memory if fully loaded: 7.90 GB\n"
          ]
        }
      ],
      "source": [
        "# Function to count rows efficiently using chunking\n",
        "def count_rows_chunked(file_path, chunk_size=10000, max_chunks=None):\n",
        "    \"\"\"Count total rows by processing in chunks\"\"\"\n",
        "    total_rows = 0\n",
        "    chunks_processed = 0\n",
        "    \n",
        "    try:\n",
        "        chunk_iter = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)\n",
        "        \n",
        "        for chunk in chunk_iter:\n",
        "            total_rows += len(chunk)\n",
        "            chunks_processed += 1\n",
        "            \n",
        "            # Limit chunks for very large files (optional)\n",
        "            if max_chunks and chunks_processed >= max_chunks:\n",
        "                print(f\"  (Limited to {max_chunks} chunks for estimation)\")\n",
        "                break\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "        return None, chunks_processed\n",
        "    \n",
        "    return total_rows, chunks_processed\n",
        "\n",
        "# Count rows for one file as example\n",
        "if test_file:\n",
        "    print(f\"Counting rows in: {test_file.name}\")\n",
        "    print(\"This may take a while for large files...\")\n",
        "    \n",
        "    row_count, chunks = count_rows_chunked(test_file, chunk_size=10000)\n",
        "    \n",
        "    if row_count is not None:\n",
        "        print(f\"Total rows: {row_count:,}\")\n",
        "        print(f\"Chunks processed: {chunks}\")\n",
        "        \n",
        "        # Estimate memory if full file loaded (approximate)\n",
        "        if sample_df is not None:\n",
        "            row_size_mb = sample_df.memory_usage(deep=True).sum() / (1024 * 1024) / len(sample_df)\n",
        "            estimated_memory_gb = (row_count * row_size_mb) / 1024\n",
        "            print(f\"Estimated memory if fully loaded: {estimated_memory_gb:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5. Row Count Comparison Across Years\n",
        "Compare row counts for all telemetry files across different years to understand data volume trends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row Count Comparison by Year:\n",
            "======================================================================\n",
            "\n",
            "2018: ALL_TELEMETRY_2018.csv\n",
            "  File size: 2398.98 MB\n",
            "  Counting rows (this may take a while)...\n",
            "  ✓ Total rows: 19,140,560\n",
            "  ✓ Chunks processed: 1915\n",
            "  ✓ Rows per MB: 7,979\n",
            "\n",
            "2019: ALL_TELEMETRY_2019.csv\n",
            "  File size: 6466.11 MB\n",
            "  Counting rows (this may take a while)...\n",
            "  ✓ Total rows: 51,628,140\n",
            "  ✓ Chunks processed: 5163\n",
            "  ✓ Rows per MB: 7,984\n",
            "\n",
            "2020: ALL_TELEMETRY_2020.csv\n",
            "  File size: 2060.52 MB\n",
            "  Counting rows (this may take a while)...\n",
            "  ✓ Total rows: 16,355,180\n",
            "  ✓ Chunks processed: 1636\n",
            "  ✓ Rows per MB: 7,937\n",
            "\n",
            "2021: ALL_TELEMETRY_2021.csv\n",
            "  File size: 6636.65 MB\n",
            "  Counting rows (this may take a while)...\n",
            "  ✓ Total rows: 52,824,160\n",
            "  ✓ Chunks processed: 5283\n",
            "  ✓ Rows per MB: 7,959\n",
            "\n",
            "2022: ALL_TELEMETRY_2022.csv\n",
            "  File size: 214.53 MB\n",
            "  Counting rows (this may take a while)...\n",
            "  ✓ Total rows: 1,725,796\n",
            "  ✓ Chunks processed: 173\n",
            "  ✓ Rows per MB: 8,044\n",
            "\n",
            "2023: ALL_TELEMETRY_2023.csv\n",
            "  File size: 5905.84 MB\n",
            "  Counting rows (this may take a while)...\n",
            "  ✓ Total rows: 47,002,000\n",
            "  ✓ Chunks processed: 4701\n",
            "  ✓ Rows per MB: 7,959\n",
            "\n",
            "2024: ALL_TELEMETRY_2024.csv\n",
            "  File size: 6205.47 MB\n",
            "  Counting rows (this may take a while)...\n",
            "  ✓ Total rows: 49,260,724\n",
            "  ✓ Chunks processed: 4927\n",
            "  ✓ Rows per MB: 7,938\n",
            "\n",
            "======================================================================\n",
            "SUMMARY - Row Counts by Year:\n",
            "======================================================================\n",
            "\n",
            "Year           Row Count     File Size (MB)      Rows/MB   % of Total\n",
            "----------------------------------------------------------------------\n",
            "2018          19,140,560           2,398.98        7,979        8.04%\n",
            "2019          51,628,140           6,466.11        7,984       21.70%\n",
            "2020          16,355,180           2,060.52        7,937        6.87%\n",
            "2021          52,824,160           6,636.65        7,959       22.20%\n",
            "2022           1,725,796             214.53        8,044        0.73%\n",
            "2023          47,002,000           5,905.84        7,959       19.75%\n",
            "2024          49,260,724           6,205.47        7,938       20.70%\n",
            "----------------------------------------------------------------------\n",
            "TOTAL        237,936,560          29,888.11        7,961       100.00%\n",
            "\n",
            "Maximum rows: 2021 with 52,824,160 rows\n",
            "Minimum rows: 2022 with 1,725,796 rows\n",
            "Range: 51,098,364 rows (2960.9% difference)\n"
          ]
        }
      ],
      "source": [
        "# Count rows for all telemetry files across years\n",
        "print(\"Row Count Comparison by Year:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "row_counts_by_year = {}\n",
        "file_sizes_by_year = {}\n",
        "\n",
        "for file_path in telemetry_files:\n",
        "    year = file_path.stem.split('_')[-1]\n",
        "    file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
        "    file_sizes_by_year[year] = file_size_mb\n",
        "    \n",
        "    print(f\"\\n{year}: {file_path.name}\")\n",
        "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
        "    print(f\"  Counting rows (this may take a while)...\")\n",
        "    \n",
        "    row_count, chunks = count_rows_chunked(file_path, chunk_size=10000)\n",
        "    \n",
        "    if row_count is not None:\n",
        "        row_counts_by_year[year] = row_count\n",
        "        print(f\"  ✓ Total rows: {row_count:,}\")\n",
        "        print(f\"  ✓ Chunks processed: {chunks}\")\n",
        "        \n",
        "        # Calculate rows per MB\n",
        "        rows_per_mb = row_count / file_size_mb if file_size_mb > 0 else 0\n",
        "        print(f\"  ✓ Rows per MB: {rows_per_mb:,.0f}\")\n",
        "    else:\n",
        "        print(f\"  ✗ Error counting rows\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUMMARY - Row Counts by Year:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "if row_counts_by_year:\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Year': list(row_counts_by_year.keys()),\n",
        "        'Row_Count': list(row_counts_by_year.values()),\n",
        "        'File_Size_MB': [file_sizes_by_year.get(year, 0) for year in row_counts_by_year.keys()]\n",
        "    })\n",
        "    \n",
        "    # Sort by year\n",
        "    comparison_df = comparison_df.sort_values('Year')\n",
        "    \n",
        "    # Calculate additional metrics\n",
        "    comparison_df['Rows_Per_MB'] = comparison_df['Row_Count'] / comparison_df['File_Size_MB']\n",
        "    comparison_df['Pct_of_Total'] = (comparison_df['Row_Count'] / comparison_df['Row_Count'].sum() * 100).round(2)\n",
        "    \n",
        "    # Display formatted table\n",
        "    print(f\"\\n{'Year':<8} {'Row Count':>15} {'File Size (MB)':>18} {'Rows/MB':>12} {'% of Total':>12}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for _, row in comparison_df.iterrows():\n",
        "        print(f\"{row['Year']:<8} {row['Row_Count']:>15,} {row['File_Size_MB']:>18,.2f} \"\n",
        "              f\"{row['Rows_Per_MB']:>12,.0f} {row['Pct_of_Total']:>11.2f}%\")\n",
        "    \n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'TOTAL':<8} {comparison_df['Row_Count'].sum():>15,} {comparison_df['File_Size_MB'].sum():>18,.2f} \"\n",
        "          f\"{comparison_df['Row_Count'].sum() / comparison_df['File_Size_MB'].sum():>12,.0f} {'100.00':>12}%\")\n",
        "    \n",
        "    # Find min/max\n",
        "    max_year = comparison_df.loc[comparison_df['Row_Count'].idxmax(), 'Year']\n",
        "    max_rows = comparison_df['Row_Count'].max()\n",
        "    min_year = comparison_df.loc[comparison_df['Row_Count'].idxmin(), 'Year']\n",
        "    min_rows = comparison_df['Row_Count'].min()\n",
        "    \n",
        "    print(f\"\\nMaximum rows: {max_year} with {max_rows:,} rows\")\n",
        "    print(f\"Minimum rows: {min_year} with {min_rows:,} rows\")\n",
        "    print(f\"Range: {max_rows - min_rows:,} rows ({((max_rows / min_rows - 1) * 100):.1f}% difference)\")\n",
        "    \n",
        "    # Store for later use\n",
        "    telemetry_row_counts = comparison_df\n",
        "else:\n",
        "    print(\"No row counts available\")\n",
        "    telemetry_row_counts = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Visualize row counts comparison (if matplotlib is available)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    if telemetry_row_counts is not None:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Plot 1: Row counts by year\n",
        "        ax1.bar(telemetry_row_counts['Year'], telemetry_row_counts['Row_Count'] / 1e6, \n",
        "                color='steelblue', alpha=0.7)\n",
        "        ax1.set_xlabel('Year', fontsize=12)\n",
        "        ax1.set_ylabel('Row Count (Millions)', fontsize=12)\n",
        "        ax1.set_title('Telemetry Row Counts by Year', fontsize=14, fontweight='bold')\n",
        "        ax1.grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for idx, row in telemetry_row_counts.iterrows():\n",
        "            height = row['Row_Count'] / 1e6\n",
        "            ax1.text(row['Year'], height, f'{height:.1f}M', \n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        # Plot 2: File size vs Row count\n",
        "        ax2.scatter(telemetry_row_counts['File_Size_MB'] / 1024, \n",
        "                   telemetry_row_counts['Row_Count'] / 1e6,\n",
        "                   s=100, alpha=0.6, color='coral')\n",
        "        ax2.set_xlabel('File Size (GB)', fontsize=12)\n",
        "        ax2.set_ylabel('Row Count (Millions)', fontsize=12)\n",
        "        ax2.set_title('File Size vs Row Count', fontsize=14, fontweight='bold')\n",
        "        ax2.grid(alpha=0.3)\n",
        "        \n",
        "        # Add year labels to scatter points\n",
        "        for idx, row in telemetry_row_counts.iterrows():\n",
        "            ax2.annotate(row['Year'], \n",
        "                        (row['File_Size_MB'] / 1024, row['Row_Count'] / 1e6),\n",
        "                        fontsize=9, ha='center')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"Visualization created successfully!\")\n",
        "    else:\n",
        "        print(\"No data available for visualization\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"Matplotlib not available - skipping visualization\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating visualization: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sampling Strategy - Get Representative Sample\n",
        "Instead of loading entire files, create a representative sample for exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get stratified sample (first N, middle N, last N rows)\n",
        "def get_stratified_sample(file_path, n_samples=5000, chunk_size=10000):\n",
        "    \"\"\"Get samples from beginning, middle, and end of file\"\"\"\n",
        "    samples = []\n",
        "    \n",
        "    try:\n",
        "        # First, count total chunks (approximate)\n",
        "        chunk_iter = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)\n",
        "        \n",
        "        chunks_list = []\n",
        "        for chunk in chunk_iter:\n",
        "            chunks_list.append(chunk)\n",
        "        \n",
        "        total_chunks = len(chunks_list)\n",
        "        \n",
        "        if total_chunks == 0:\n",
        "            return None\n",
        "        \n",
        "        # Sample from beginning\n",
        "        if len(chunks_list[0]) > 0:\n",
        "            samples.append(chunks_list[0].head(min(n_samples // 3, len(chunks_list[0]))))\n",
        "        \n",
        "        # Sample from middle\n",
        "        if total_chunks > 2:\n",
        "            mid_chunk_idx = total_chunks // 2\n",
        "            if len(chunks_list[mid_chunk_idx]) > 0:\n",
        "                samples.append(chunks_list[mid_chunk_idx].head(min(n_samples // 3, len(chunks_list[mid_chunk_idx]))))\n",
        "        \n",
        "        # Sample from end\n",
        "        if total_chunks > 1:\n",
        "            if len(chunks_list[-1]) > 0:\n",
        "                samples.append(chunks_list[-1].tail(min(n_samples // 3, len(chunks_list[-1]))))\n",
        "        \n",
        "        # Combine samples\n",
        "        if samples:\n",
        "            combined_sample = pd.concat(samples, ignore_index=True)\n",
        "            # Remove duplicates if any\n",
        "            combined_sample = combined_sample.drop_duplicates()\n",
        "            return combined_sample\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error sampling {file_path.name}: {e}\")\n",
        "        return None\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Get sample for exploration\n",
        "if test_file:\n",
        "    print(f\"Creating sample from: {test_file.name}\")\n",
        "    print(\"Sampling 5,000 rows (stratified across file)...\")\n",
        "    \n",
        "    sample_telemetry = get_stratified_sample(test_file, n_samples=5000, chunk_size=10000)\n",
        "    \n",
        "    if sample_telemetry is not None:\n",
        "        print(f\"\\nSample size: {len(sample_telemetry):,} rows, {len(sample_telemetry.columns)} columns\")\n",
        "        print(f\"Sample memory usage: {sample_telemetry.memory_usage(deep=True).sum() / (1024 * 1024):.2f} MB\")\n",
        "        \n",
        "        print(\"\\nSample data preview:\")\n",
        "        print(sample_telemetry.head(10))\n",
        "        \n",
        "        print(\"\\nData types:\")\n",
        "        print(sample_telemetry.dtypes)\n",
        "        \n",
        "        print(\"\\nMissing values:\")\n",
        "        missing = sample_telemetry.isnull().sum()\n",
        "        print(missing[missing > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary Statistics by Year (Chunked Processing)\n",
        "Get summary statistics for each year without loading full files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get summary stats chunked\n",
        "def get_summary_stats_chunked(file_path, numeric_cols=None, chunk_size=50000, max_chunks=10):\n",
        "    \"\"\"Calculate summary statistics by processing chunks\"\"\"\n",
        "    stats_list = []\n",
        "    row_counts = []\n",
        "    \n",
        "    try:\n",
        "        chunk_iter = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False, usecols=numeric_cols)\n",
        "        \n",
        "        for i, chunk in enumerate(chunk_iter):\n",
        "            if max_chunks and i >= max_chunks:\n",
        "                break\n",
        "            \n",
        "            # Basic stats\n",
        "            row_counts.append(len(chunk))\n",
        "            \n",
        "            # Numeric summary\n",
        "            if numeric_cols:\n",
        "                numeric_chunk = chunk.select_dtypes(include=[np.number])\n",
        "                if len(numeric_chunk.columns) > 0:\n",
        "                    stats_list.append(numeric_chunk.describe())\n",
        "        \n",
        "        if stats_list:\n",
        "            # Combine stats (approximate - averaging across chunks)\n",
        "            combined_stats = pd.concat(stats_list).groupby(level=0).mean()\n",
        "            total_rows = sum(row_counts)\n",
        "            return combined_stats, total_rows\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "        return None, 0\n",
        "    \n",
        "    return None, sum(row_counts)\n",
        "\n",
        "# Get summary for all years (limited processing)\n",
        "print(\"Summary Statistics (Limited Processing):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for file_path in telemetry_files[:3]:  # Process first 3 files only as example\n",
        "    year = file_path.stem.split('_')[-1]\n",
        "    print(f\"\\n{year}: {file_path.name}\")\n",
        "    \n",
        "    # First get numeric columns from sample\n",
        "    info, _ = get_columns_info(file_path, chunk_size=1000)\n",
        "    if info:\n",
        "        numeric_cols = [col for col, dtype in info['dtypes'].items() \n",
        "                       if pd.api.types.is_numeric_dtype(dtype)]\n",
        "        \n",
        "        print(f\"  Processing up to 10 chunks...\")\n",
        "        stats, row_count = get_summary_stats_chunked(\n",
        "            file_path, \n",
        "            numeric_cols=numeric_cols[:10] if len(numeric_cols) > 10 else numeric_cols,  # Limit columns\n",
        "            chunk_size=50000,\n",
        "            max_chunks=10\n",
        "        )\n",
        "        \n",
        "        print(f\"  Rows processed: {row_count:,}\")\n",
        "        if stats is not None:\n",
        "            print(f\"  Numeric columns analyzed: {len(stats.columns)}\")\n",
        "            print(f\"  Sample stats:\\n{stats.head()}\")\n",
        "    \n",
        "    # Clear memory\n",
        "    if 'stats' in locals():\n",
        "        del stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Insights and Data Structure\n",
        "Summarize findings for feature extraction planning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary document\n",
        "if sample_telemetry is not None:\n",
        "    print(\"TELEMETRY DATA STRUCTURE SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nColumns ({len(sample_telemetry.columns)}):\")\n",
        "    for col in sample_telemetry.columns:\n",
        "        dtype = sample_telemetry[col].dtype\n",
        "        null_pct = (sample_telemetry[col].isnull().sum() / len(sample_telemetry)) * 100\n",
        "        print(f\"  - {col:30s} {str(dtype):15s} {null_pct:5.1f}% null\")\n",
        "    \n",
        "    print(f\"\\nUnique values in categorical columns:\")\n",
        "    for col in sample_telemetry.select_dtypes(include=['object']).columns:\n",
        "        unique_count = sample_telemetry[col].nunique()\n",
        "        if unique_count < 20:  # Only show if reasonable number of unique values\n",
        "            print(f\"  - {col:30s}: {unique_count} unique values\")\n",
        "            print(f\"    Values: {list(sample_telemetry[col].unique())[:10]}\")\n",
        "    \n",
        "    print(f\"\\nNumeric columns ranges:\")\n",
        "    for col in sample_telemetry.select_dtypes(include=[np.number]).columns:\n",
        "        print(f\"  - {col:30s}: [{sample_telemetry[col].min():.2f}, {sample_telemetry[col].max():.2f}]\")\n",
        "    \n",
        "    print(f\"\\nTime/Date columns:\")\n",
        "    time_cols = [col for col in sample_telemetry.columns \n",
        "                if 'time' in col.lower() or 'date' in col.lower()]\n",
        "    for col in time_cols:\n",
        "        print(f\"  - {col}\")\n",
        "        print(f\"    Sample values: {sample_telemetry[col].head(3).tolist()}\")\n",
        "    \n",
        "    # Memory recommendations\n",
        "    if test_file:\n",
        "        file_size_mb = test_file.stat().st_size / (1024 * 1024)\n",
        "        if file_size_mb > 100:\n",
        "            print(f\"\\n⚠️  MEMORY WARNING:\")\n",
        "            print(f\"   File size: {file_size_mb:.2f} MB\")\n",
        "            print(f\"   Recommendation: Use chunked processing for feature extraction\")\n",
        "            print(f\"   Suggested chunk size: 10,000 - 50,000 rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Memory-Efficient Feature Planning\n",
        "Based on exploration, plan feature extraction strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function for chunked feature extraction (template)\n",
        "def process_telemetry_chunked(file_path, feature_func, chunk_size=50000, output_path=None):\n",
        "    \"\"\"\n",
        "    Template for memory-efficient feature extraction from telemetry data.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : Path\n",
        "        Path to telemetry CSV file\n",
        "    feature_func : callable\n",
        "        Function that takes a chunk DataFrame and returns features\n",
        "    chunk_size : int\n",
        "        Number of rows to process per chunk\n",
        "    output_path : Path, optional\n",
        "        Path to save extracted features\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    try:\n",
        "        chunk_iter = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)\n",
        "        \n",
        "        for i, chunk in enumerate(chunk_iter):\n",
        "            # Extract features from chunk\n",
        "            features = feature_func(chunk)\n",
        "            results.append(features)\n",
        "            \n",
        "            # Optional: Save incrementally to disk\n",
        "            if output_path and (i + 1) % 10 == 0:  # Save every 10 chunks\n",
        "                temp_df = pd.concat(results, ignore_index=True)\n",
        "                temp_df.to_csv(output_path, mode='a', header=not output_path.exists(), index=False)\n",
        "                results = []  # Clear from memory\n",
        "                print(f\"  Saved progress after {i + 1} chunks\")\n",
        "        \n",
        "        # Final save\n",
        "        if results:\n",
        "            final_df = pd.concat(results, ignore_index=True)\n",
        "            if output_path:\n",
        "                final_df.to_csv(output_path, mode='a', header=not output_path.exists(), index=False)\n",
        "            else:\n",
        "                return final_df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path.name}: {e}\")\n",
        "        return None\n",
        "    \n",
        "    return None\n",
        "\n",
        "print(\"Feature extraction template function created.\")\n",
        "print(\"\\nUsage example:\")\n",
        "print(\"  def extract_features(chunk):\")\n",
        "print(\"      # Your feature extraction logic here\")\n",
        "print(\"      return features_df\")\n",
        "print(\"  \")\n",
        "print(\"  process_telemetry_chunked(\")\n",
        "print(\"      file_path=telemetry_file,\")\n",
        "print(\"      feature_func=extract_features,\")\n",
        "print(\"      chunk_size=50000,\")\n",
        "print(\"      output_path=Path('features.csv')\")\n",
        "print(\"  )\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
